{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ea4d495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "637e9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return enc.encode(text)\n",
    "\n",
    "def split_text_to_chunks(text: str, chunk_size:int=500) -> List[str]:\n",
    "    tokens = count_tokens(text)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "598f2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', max_completion_tokens=300, temperature=0, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9b7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\n",
    "<chunk>\n",
    "{chunk_text}\n",
    "</chunk>\n",
    "\n",
    "You are summarizing a chunk of a larger document.\n",
    "The summary must:\n",
    "- Briefly capture the main idea of the chunk (max 300 tokens).\n",
    "- Clearly link the chunk to the broader document context.\n",
    "- Avoid filler, repetition, or generic phrases.\n",
    "- Be standalone so it can be stored and retrieved without needing the full document.\n",
    "\n",
    "Write exactly ONE concise paragraph.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3281f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_summarize_body(body_list: List[str], tpm_limit:int=180000) -> List[List[str]]:\n",
    "    tasks: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    for i, body_text in enumerate(body_list):\n",
    "        if not body_text:\n",
    "            continue\n",
    "\n",
    "        chunks = split_text_to_chunks(body_text)\n",
    "        for chunk in chunks:\n",
    "            formatted_prompt = chunk_prompt_template.format(document_text=body_text, chunk_text=chunk)\n",
    "            token_est = count_tokens(formatted_prompt)\n",
    "            tasks.append((i, HumanMessage(content=formatted_prompt), token_est))\n",
    "\n",
    "    if not tasks:\n",
    "        return [[] for _ in body_list]\n",
    "    \n",
    "    summaries: List[List[str]] = [[] for _ in body_list]\n",
    "    current_batch: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    current_tokens = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    def run_batch(batch: List[Tuple[int, List[HumanMessage], int]]):\n",
    "        responses = llm.batch([msg for _, msg, _ in batch])\n",
    "        for (doc_index, _, _), resp in zip(batch, responses):\n",
    "            summaries[doc_index].append(resp.content.strip())\n",
    "\n",
    "    for task in tasks:\n",
    "        _, _, token = task\n",
    "        if current_tokens + token > tpm_limit and current_batch:\n",
    "            run_batch(current_batch)\n",
    "            current_batch, current_tokens = [], 0\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed < 30:\n",
    "                time.sleep(30-elapsed)\n",
    "            start_time = time.time()\n",
    "\n",
    "        current_batch.append(task)\n",
    "        current_tokens += token\n",
    "\n",
    "    if current_batch:\n",
    "        run_batch(current_batch)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "928a3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert email summarizer. Generate ONE professional paragraph summarizing the email.\n",
    "STRICT RULES:\n",
    "1. You MUST include these fields exactly once: Email_id, Thread_id, From, To, CC, Source, Date, Labels, Subject.\n",
    "2. You MUST reproduce every field value exactly as provided, without paraphrasing or omission.\n",
    "3. You MUST explicitly list EVERY recipient in \"To\" and \"CC\". Do NOT use 'others', 'etc.', or shorten lists.\n",
    "4. If a field is empty or 'None', output the word \"None\".\n",
    "5. Write as a single paragraph, grammatically correct, no bullet points.\n",
    "6. The output MUST follow this format:\n",
    "Email Email_id, part of thread Thread_id, was sent by From. The To recipients are To. The CC's are CC. It originated from Source on Date. The email carries the labels Labels. The subject of the email is 'Subject'.\n",
    "\n",
    "Here is the metadata for the email:\n",
    "{metadata}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "def get_metadata_summaries(metadata_list: List[str], tpm_limit:int = 180000) -> List[str]:\n",
    "    meta_tasks = List[Tuple[List[HumanMessage], int]] = []\n",
    "    for md in metadata_list:\n",
    "        formatted_prompt = meta_data_prompt.format(metadata=md)\n",
    "        token_est = count_tokens(formatted_prompt)\n",
    "        meta_tasks.append((HumanMessage(content=formatted_prompt), token_est))\n",
    "\n",
    "    if not meta_tasks:\n",
    "        return []\n",
    "    \n",
    "    meta_summaries: List[str] = []\n",
    "    \n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    def run_batch(batch: List[Tuple[List[HumanMessage], int]]):\n",
    "        responses = llm.batch([msg for msg, _ in batch])\n",
    "        for resp in responses:\n",
    "            meta_summaries.append(resp.content.strip())\n",
    "\n",
    "    for task in meta_tasks:\n",
    "        _, _, token = task\n",
    "        if current_tokens + token > tpm_limit and current_batch:\n",
    "            run_batch(current_batch)\n",
    "            current_batch, current_tokens = [], 0\n",
    "\n",
    "            elapsed = time.time()-start_time\n",
    "            if elapsed<30:\n",
    "                time.sleep(30-start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "        current_batch.append(task)\n",
    "        current_tokens += token\n",
    "\n",
    "    if current_batch:\n",
    "        run_batch(current_batch)\n",
    "\n",
    "    return meta_summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
