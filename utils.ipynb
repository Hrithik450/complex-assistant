{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed11fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"d:\\VSCode\\re-assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a46655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command-line environment detected. Using local data file.\n",
      "Loading email metadata from: d:\\VSCode\\re-assistant\\lib\\data\\all_mails.jsonl\n",
      "Successfully loaded 11807 records for metadata.\n",
      "Connecting to ChromaDB Vector Store...\n",
      "Successfully connected to ChromaDB collection.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "import redis\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from lib.utils import AGENT_MODEL, SYSTEM_PROMPT\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from lib.db.db_service import ThreadService\n",
    "# from lib.db.db_conn import conn\n",
    "from datetime import datetime\n",
    "from lib.load_data import df\n",
    "from rapidfuzz import fuzz\n",
    "from lib.utils import match_value_in_columns, normalize_email_field, normalize_list\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d020899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import polars as pl\n",
    "from langchain.tools import tool\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.clone()\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "])\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "])\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "])\n",
    "\n",
    "print(temp_df['from_normalized'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da609c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def build_name_dict(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorized, memory-efficient building of:\n",
    "        token -> {\"full\": full_name, \"emails\": [list of emails]}\n",
    "\n",
    "    Uses DataFrame.unpivot (replacement for deprecated melt), explode, and Polars string ops.\n",
    "    \"\"\"\n",
    "    cols = [c for c in [\"from_normalized\", \"to_normalized\", \"cc_normalized\"] if c in df.columns]\n",
    "    if not cols:\n",
    "        raise ValueError(\"No normalized columns found. Expect one of: from_normalized, to_normalized, cc_normalized\")\n",
    "\n",
    "    # 1) unpivot (stack the normalized columns into a single column \"addr\")\n",
    "    stacked = df.unpivot(index=[], on=cols, variable_name=\"src\", value_name=\"addr\")\n",
    "\n",
    "    stacked = stacked.filter(pl.col(\"addr\") != \"\")\n",
    "\n",
    "    stacked = stacked.with_columns(\n",
    "        pl.col(\"addr\").str.split(\",\").alias(\"addr_list\")\n",
    "    )\n",
    "\n",
    "    stacked = stacked.explode(\"addr_list\")\n",
    "\n",
    "    stacked = stacked.with_columns(\n",
    "        pl.col(\"addr_list\").str.strip_chars().alias(\"addr\")\n",
    "    ).drop(\"addr_list\")\n",
    "\n",
    "    stacked = stacked.filter(\n",
    "        pl.col(\"addr\").is_first_distinct().alias(\"unique_addr\")\n",
    "    )\n",
    "    \n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d13977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = build_name_dict(temp_df)\n",
    "names_series = stacked['addr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acaf7700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2_646,)\n",
      "Series: 'addr' [str]\n",
      "[\n",
      "\t\"harish.sales harish.sales@ahlaâ€¦\n",
      "\t\"contact 2getherments infra pvtâ€¦\n",
      "\t\"balakrishna info@2getherments.â€¦\n",
      "\t\"malini satish kumar malini.satâ€¦\n",
      "\t\"customer communications 2g cx â€¦\n",
      "\tâ€¦\n",
      "\t\"neeti1919@gmail.com\"\n",
      "\t\"nwajit@gmail.com\"\n",
      "\t\"arshkaur19@gmail.com\"\n",
      "\t\"anjalisinha373@gmail.com\"\n",
      "\t\"anindita92nayak@gmail.com\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(names_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a91a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json(raw_response):\n",
    "    if not raw_response:\n",
    "        return None\n",
    "    match = re.search(r'\\{.*\\}', raw_response, re.S)\n",
    "    if match:\n",
    "        return json.loads(match.group(0))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de57a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176b9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0, max_tokens=300, api_key='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')\n",
    "\n",
    "def run_batch_task(tasks: List[Tuple[int, List[HumanMessage], int]], tpm_limit: int = 180000) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    tasks: list of (task_id, messages, est_tokens)\n",
    "    tpm_limit: max tokens/minute allowed\n",
    "    returns: list of (task_id, response_text)\n",
    "    \"\"\"\n",
    "    results: List[Tuple[int, str]] = []\n",
    "    current_batch: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    current_tokens = 0\n",
    "    window_start = time.time()\n",
    "\n",
    "    def flush(batch):\n",
    "      \"\"\"Send a batch to the LLM and record results.\"\"\"\n",
    "      print(f\"\\nðŸš€ Flushing {len(batch)} tasks \"\n",
    "              f\"({sum(tok for _, _, tok in batch)} tokens)...\")\n",
    "\n",
    "      responses = llm.batch([msgs for _, msgs, _ in batch])\n",
    "      for (task_id, _, _), resp in zip(batch, responses):\n",
    "        print(f\"   âœ… Task {task_id} completed.\")\n",
    "        results.append((task_id, resp.content))\n",
    "\n",
    "    for task in tasks:\n",
    "      _, _, tok = task\n",
    "\n",
    "      if current_tokens + tok > tpm_limit and current_batch:\n",
    "        flush(current_batch)\n",
    "        current_batch, current_tokens = [], 0\n",
    "\n",
    "        elapsed = time.time() - window_start\n",
    "        if elapsed < 60:\n",
    "          time.sleep(60 - elapsed)\n",
    "        window_start = time.time()\n",
    "\n",
    "      current_batch.append(task)\n",
    "      current_tokens += tok\n",
    "    \n",
    "    if current_batch:\n",
    "      flush(current_batch)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c64f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from langchain.schema  import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an information extraction system.  \n",
    "\n",
    "Task:  \n",
    "- Extract only meaningful PERSON NAME (not initials like 1-2 letters) tokens from the given string.\n",
    "- Extract **every EMAIL address as a token without exception.**\n",
    "- Emails must always be included, even if they contain numbers, company names, or other patterns.\n",
    "- Split multi-part names into separate tokens (e.g., \"Satish Kumar s\" â†’ [\"satish\", \"kumar\"]).\n",
    "- Ignore company suffixes or terms (e.g., \"Ltd\", \"Inc\", \"Pvt\", \"Sales\"), standalone numbers, and generic stop-words.\n",
    "- Convert all tokens to lowercase.\n",
    "- Return the result as a **strict JSON object only**, with no explanations, extra text, or formatting.\n",
    "- Always return in the exact format below, with valid JSON only (no trailing commas, no comments).\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "  \"tokens\": [\"\", \"\"]\n",
    "}}\n",
    "\n",
    "Examples:  \n",
    "Input: \"customer cx customer.communications@2getherments.com\"  \n",
    "Output: {{ \"tokens\": [\"customer\", \"customer.communications@2getherments.com\"] }}\n",
    "\n",
    "Input: \"213 rahul sinha rahulsinha198@gmail.com\"  \n",
    "Output: {{ \"tokens\": [\"rahul\", \"sinha\", \"rahulsinha198@gmail.com\"] }}\n",
    "\n",
    "Input: \"pavan hs hspavankumar@yahoo.com\"  \n",
    "Output: {{ \"tokens\": [\"pavan\", \"hspavankumar@yahoo.com\"] }}\n",
    "\n",
    "Now process this input:  \n",
    "Full string: {full_name}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "tasks:List[Tuple[int, List[HumanMessage], int]] = []\n",
    "\n",
    "for idx, full_name in enumerate(names_series):\n",
    "  formatted_prompt = prompt_template.format(full_name=full_name)\n",
    "  token_est = count_tokens(formatted_prompt)\n",
    "  messages = [HumanMessage(content=formatted_prompt)]\n",
    "  tasks.append((idx, messages, token_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33e27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_batch_task(tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44beced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, resp in results:\n",
    "    print(task_id, parse_json(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2855c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(\"lib/data/name_token_map.jsonl\")\n",
    "with out_path.open(\"w\", encoding='utf-8') as f:\n",
    "    for task_id, resp in results:\n",
    "        try:\n",
    "            parser_json = parse_json(resp)\n",
    "            f.write(json.dumps({\n",
    "                \"full_name\": names_series[task_id],\n",
    "                \"tokens\": parser_json['tokens']\n",
    "            }) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Bad JSON for row {task_id}: {resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f421d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file  = Path(\"lib/data/name_token_map.jsonl\")\n",
    "output_file = Path(\"lib/data/token_map.jsonl\")\n",
    "\n",
    "# Step 1: Build token_map\n",
    "token_map = defaultdict(set)\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        full_name = entry[\"full_name\"].strip()   # remove leading/trailing spaces\n",
    "        tokens = entry[\"tokens\"]\n",
    "\n",
    "        for token in tokens:\n",
    "            # Optional: normalize token if needed\n",
    "            normalized_token = token.strip().lower()\n",
    "            token_map[normalized_token].add(full_name)\n",
    "\n",
    "# Step 2: Write out as JSONL (deduplicated automatically via set)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for token, names in token_map.items():\n",
    "        # Sort for consistency (optional)\n",
    "        unique_names = sorted(names)\n",
    "        f.write(json.dumps({token: unique_names}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b97a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "stacked = build_name_dict(temp_df)\n",
    "names_series = stacked['addr']\n",
    "\n",
    "token_map = defaultdict(set)\n",
    "word_re = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)*\")\n",
    "for full in names_series:\n",
    "    cleaned = re.sub(\n",
    "        r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\n",
    "        \"\",\n",
    "        full\n",
    "    )\n",
    "    for token in word_re.findall(cleaned.lower()):\n",
    "        token_map[token].add(full)\n",
    "\n",
    "out_file = Path(\"lib/data/name_token_map.jsonl\")\n",
    "with out_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for token, fulls in token_map.items():\n",
    "        f.write(json.dumps({\"token\": token, \"full_names\": list(fulls)}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c285361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Set\n",
    "import json\n",
    "\n",
    "token_map: Dict[str, Set[str]] = defaultdict(set)\n",
    "\n",
    "with open(\"lib/data/token_map.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        for token, names in entry.items():\n",
    "            token_map[token].update(names)\n",
    "\n",
    "token_map = dict(token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff0a14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Set\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "WORD_RE = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)*\")\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    \"\"\"Normalize separators for comparison.\"\"\"\n",
    "    return re.sub(r\"[-_]+\", \" \", s.lower()).strip()\n",
    "\n",
    "def expand_query(query: str, token_map: Dict[str, Set[str]], fuzzy_threshold: float = 0.75) -> str:\n",
    "    if not token_map:\n",
    "        return query\n",
    "\n",
    "    query_tokens = WORD_RE.findall(query)\n",
    "    expanded_tokens = []\n",
    "\n",
    "    for q_tok in query_tokens:\n",
    "        q_norm = normalize_token(q_tok)\n",
    "        best_full = None\n",
    "        best_score = 0.0\n",
    "\n",
    "        for token, full_names in token_map.items():\n",
    "            token_norm = normalize_token(token)\n",
    "            for full in full_names:\n",
    "                # Compare normalized query token against token and full name\n",
    "                sim_token = SequenceMatcher(None, q_norm, token_norm).ratio()\n",
    "                sim_full = SequenceMatcher(None, q_norm, normalize_token(full)).ratio()\n",
    "                sim = max(sim_token, sim_full)\n",
    "\n",
    "                if sim > best_score:\n",
    "                    best_score = sim\n",
    "                    best_full = full\n",
    "\n",
    "        if best_full and best_score >= fuzzy_threshold:\n",
    "            expanded_tokens.append(best_full)\n",
    "        else:\n",
    "            expanded_tokens.append(q_tok)\n",
    "\n",
    "    return \" \".join(expanded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "519882c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need details msme chamber of commerce and industry of india msmeglobalccii@msmeccii.in sankar narayanan sankar.narayanan@2getherments.com gvvsl narayana gvvslnarayana@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(expand_query(query=\"need details of sankar narayan\", token_map=token_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc44bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from typing import Dict, Set, Optional, Tuple\n",
    "\n",
    "_HAVE_RAPIDFUZZ = True\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # remove angle/round-bracketed extras and email localparts\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\S+@\\S+\", \" \", s)\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "\n",
    "    sender_norm = _normalize(sender)\n",
    "\n",
    "    # Fast path: exact token key match (case-insensitive)\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: _score(sender_norm, _normalize(f))\n",
    "                )\n",
    "                best_score = _score(sender_norm, _normalize(best_full))\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "\n",
    "    # Build candidate list (normalized strings) -> meta mapping\n",
    "    candidates = []\n",
    "    meta = {}  # candidate_norm -> (token_key, full_name_or_None)\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = _normalize(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = _normalize(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # If rapidfuzz available, use extractOne with a strong scorer (WRatio)\n",
    "    if _HAVE_RAPIDFUZZ:\n",
    "        # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "        unique_choices = list(dict.fromkeys(candidates))\n",
    "        match = process.extractOne(\n",
    "            sender_norm,\n",
    "            unique_choices,\n",
    "            scorer=fuzz.WRatio,\n",
    "            score_cutoff=threshold\n",
    "        )\n",
    "        if not match:\n",
    "            return None\n",
    "        match_str, score, _ = match  # match_str is normalized candidate\n",
    "        token_key, full_name = meta.get(match_str, (None, None))\n",
    "        # If the match candidate was just a token key (full_name is None),\n",
    "        # pick the best full_name under that token_key\n",
    "        if full_name is None and token_key is not None:\n",
    "            best_full = max(\n",
    "                token_map[token_key],\n",
    "                key=lambda f: fuzz.WRatio(sender_norm, _normalize(f))\n",
    "            )\n",
    "            best_score = fuzz.WRatio(sender_norm, _normalize(best_full))\n",
    "            return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "        return (full_name, float(score))\n",
    "\n",
    "    # Fallback: iterate and use SequenceMatcher ratio\n",
    "    best_full = None\n",
    "    best_score = 0.0\n",
    "    for token_key, full_names in token_map.items():\n",
    "        token_norm = _normalize(token_key)\n",
    "        # score against token key\n",
    "        token_score = max(_seq_ratio(sender_norm, token_norm), 0.0)\n",
    "        if token_score * 100 > best_score:\n",
    "            # if token seems promising check its full names\n",
    "            for full in full_names:\n",
    "                full_norm = _normalize(full)\n",
    "                s = _seq_ratio(sender_norm, full_norm) * 100\n",
    "                if s > best_score:\n",
    "                    best_score = s\n",
    "                    best_full = full\n",
    "        # also compare sender directly to each full_name\n",
    "        for full in full_names:\n",
    "            full_norm = _normalize(full)\n",
    "            s = _seq_ratio(sender_norm, full_norm) * 100\n",
    "            if s > best_score:\n",
    "                best_score = s\n",
    "                best_full = full\n",
    "\n",
    "    if best_full and best_score >= threshold:\n",
    "        return best_full, best_score\n",
    "    return None\n",
    "\n",
    "# helper scoring functions (used by fallback and for small composition)\n",
    "def _seq_ratio(a: str, b: str) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()  # returned 0..1\n",
    "\n",
    "def _score(a_norm: str, b_norm: str) -> float:\n",
    "    \"\"\"Return 0..100 score using rapidfuzz if present, else difflib*100.\"\"\"\n",
    "    if _HAVE_RAPIDFUZZ:\n",
    "        return float(fuzz.WRatio(a_norm, b_norm))\n",
    "    return _seq_ratio(a_norm, b_norm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "722babd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # remove angle/round-bracketed extras and email localparts\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\S+@\\S+\", \" \", s)\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def score(a_norm: str, b_norm: str) -> float:\n",
    "    \"\"\"Return 0..100 score using rapidfuzz if present, else difflib*100.\"\"\"\n",
    "    return float(fuzz.WRatio(a_norm, b_norm))\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "    sender_norm = normalize_text(sender)\n",
    "\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: score(sender_norm, normalize_text(f))\n",
    "                )\n",
    "                best_score = score(sender_norm, normalize_text(best_full))\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "    \n",
    "    candidates = []\n",
    "    meta = {}\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = normalize_text(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = normalize_text(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "    unique_choices = list(dict.fromkeys(candidates))\n",
    "    match = process.extractOne(\n",
    "        sender_norm,\n",
    "        unique_choices,\n",
    "        scorer=fuzz.WRatio,\n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    if not match:\n",
    "        return None\n",
    "    match_str, score, _ = match  # match_str is normalized candidate\n",
    "    token_key, full_name = meta.get(match_str, (None, None))\n",
    "    # If the match candidate was just a token key (full_name is None),\n",
    "    # pick the best full_name under that token_key\n",
    "    if full_name is None and token_key is not None:\n",
    "        best_full = max(\n",
    "            token_map[token_key],\n",
    "            key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))\n",
    "        )\n",
    "        best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))\n",
    "        return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "    return (full_name, float(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "from typing import Dict, Set, Optional, Tuple\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "    sender_norm = normalize_text(sender)\n",
    "\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))  # fixed\n",
    "                )\n",
    "                best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))  # fixed\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "    \n",
    "    candidates = []\n",
    "    meta = {}\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = normalize_text(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = normalize_text(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "    unique_choices = list(dict.fromkeys(candidates))\n",
    "    match = process.extractOne(\n",
    "        sender_norm,\n",
    "        unique_choices,\n",
    "        scorer=fuzz.WRatio,\n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    if not match:\n",
    "        return None\n",
    "    match_str, score, _ = match  # match_str is normalized candidate\n",
    "    token_key, full_name = meta.get(match_str, (None, None))\n",
    "    # If the match candidate was just a token key (full_name is None),\n",
    "    # pick the best full_name under that token_key\n",
    "    if full_name is None and token_key is not None:\n",
    "        best_full = max(\n",
    "            token_map[token_key],\n",
    "            key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))\n",
    "        )\n",
    "        best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))\n",
    "        return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "    return (full_name, float(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df447647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('venkata kishore kalagotla via team 2getherments team.2getherments@2getherments.com', 90.0)\n"
     ]
    }
   ],
   "source": [
    "sender = 'kishore kalagotla'\n",
    "if sender:\n",
    "    sender = sender.lower()\n",
    "    res = get_best_match_from_token_map(sender, token_map, threshold=75)\n",
    "    if res:\n",
    "        print(res)\n",
    "        best_full_name, score = res\n",
    "        sender = best_full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee17f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "989d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = \"kishore kalagotla 2getherments\"\n",
    "\n",
    "mask = pl.lit(True)\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "])\n",
    "\n",
    "sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "mask = mask & sender_mask\n",
    "\n",
    "temp_df = temp_df.filter(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "812e75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kishore kalagotla  kishore.kalagotla@2getherments.com\n"
     ]
    }
   ],
   "source": [
    "print(temp_df['from_normalized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4bfdaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "value = \"sankar.narayanan@2getherments.com\"\n",
    "column = \"sankar narayanan sankar.narayanan@2getherments.com\"\n",
    "\n",
    "print(fuzz.partial_ratio(value.lower(), column.lower()) > 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b850dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import normalize_list, match_value_in_columns, get_best_match_from_token_map\n",
    "from lib.load_data import df, token_map\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04663d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_subject(subject: str) -> str:\n",
    "    if not isinstance(subject, str):\n",
    "        return \"\"\n",
    "    subject = re.sub(r'^(re|fwd|fw):\\s*', '', subject, flags=re.I)  # remove reply/forward\n",
    "    return subject.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f4ec47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refund statement - 805\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_subject(\"Re: Refund Statement - 805\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07f7f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(text: str) -> set[str]:\n",
    "    return set(re.findall(r'\\b\\d+\\b', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fb44a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'805'}\n"
     ]
    }
   ],
   "source": [
    "print(extract_numbers(\"refund statement - 805\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3cedc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def preprocess_subject(subject: str) -> str:\n",
    "    if not isinstance(subject, str):\n",
    "        return \"\"\n",
    "    # Lowercase and replace symbols with space\n",
    "    subject = re.sub(r'[:\\-_,]', ' ', subject)\n",
    "    subject = re.sub(r'\\s+', ' ', subject)  # normalize spaces\n",
    "    return subject.lower().strip()\n",
    "\n",
    "def extract_numbers(text: str) -> set[str]:\n",
    "    return set(re.findall(r'\\b\\d+\\b', text))\n",
    "\n",
    "def smart_subject_match(user_value: str, column_value: str) -> bool:\n",
    "    if not column_value:\n",
    "        return False\n",
    "    \n",
    "    user_clean = preprocess_subject(user_value)\n",
    "    col_clean = preprocess_subject(column_value)\n",
    "\n",
    "    user_nums = extract_numbers(user_clean)\n",
    "    col_nums = extract_numbers(col_clean)\n",
    "\n",
    "    # --- Number must match if present ---\n",
    "    if user_nums and not (user_nums & col_nums):\n",
    "        return False\n",
    "\n",
    "    # --- Fuzzy match on remaining text ---\n",
    "    fuzz_score = fuzz.token_set_ratio(user_clean, col_clean) / 100\n",
    "\n",
    "    if user_nums:\n",
    "        # numbers match â†’ relax threshold\n",
    "        return fuzz_score >= 0.65\n",
    "    else:\n",
    "        # no numbers â†’ require stricter match\n",
    "        return fuzz_score >= 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "efbae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pl.lit(True)\n",
    "subject = \"Refund Statement 805\"\n",
    "# \"Refund for Flat no: 805\"\n",
    "# \"Re: Refund Statement - 805\"\n",
    "temp_df = df.clone()\n",
    "\n",
    "if subject:    \n",
    "    subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "    mask = mask & subject_mask\n",
    "\n",
    "temp_df = temp_df.filter(mask)\n",
    "\n",
    "total_matches = temp_df.height\n",
    "preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "\n",
    "results_preview = temp_df.head(5).select(preview_cols).to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb922765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '198f083a5ce11830', 'threadId': '198f02e745824fb8', 'from': 'Sankar Narayanan <sankar.narayanan@2getherments.com>', 'to': ['nirav.j05@gmail.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T17:20:03+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': '@Rama Krishna Please proceed further. On Thu, Aug 28, 2025 at 5:05 PM Nirav Joshi &lt;nirav.j05@gmail.com&gt; wrote: Thank you Sankar sir for your swift response. Please proceed with the same. On Thu,', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '199098a3358031c1', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-09-02T13:57:42+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com', 'arshkaur19@gmail.com'], 'snippet': 'Hi Rama sir, Please update status on this request. Thank you. On Thu, Aug 28, 2025 at 5:20 PM Sankar Narayanan &lt;sankar.narayanan@2getherments.com&gt; wrote: @Rama Krishna Please proceed further. On', 'labels': ['IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f0418716eeeef', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T16:07:48+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Dear Sankar sir, Can you please share the Excel file and there is Total missing in above email can you please make corrections and share with us. On Thu, Aug 28, 2025 at 3:47 PM Sankar Narayanan &lt;', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f02e745824fb8', 'threadId': '198f02e745824fb8', 'from': 'Sankar Narayanan <sankar.narayanan@2getherments.com>', 'to': ['nirav.j05@gmail.com'], 'subject': 'Refund Statement - 805', 'date': '2025-08-28T15:47:00+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Dear Nirav, As discussed,Please find the refund statement attached for your reference. Kindly Acknowledge to proceed further. Shankar.S Manager-Customer Relations Ph:63669 55995 805 Refund Statement S.', 'labels': ['UNREAD', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f075c6bc02c3c', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T17:04:52+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Thank you Sankar sir for your swift response. Please proceed with the same. On Thu, Aug 28, 2025 at 4:07 PM Nirav Joshi &lt;nirav.j05@gmail.com&gt; wrote: Dear Sankar sir, Can you please share the', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for result in results_preview:\n",
    "    print(result)\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb97eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import normalize_list, match_value_in_columns, smart_subject_match\n",
    "from lib.load_data import df\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl\n",
    "from typing import Union\n",
    "\n",
    "# @tool(\"email_filtering_tool\", parse_docstring=True)\n",
    "def email_filtering_tool(\n",
    "    uid: str = None,\n",
    "    threadId: str = None,\n",
    "    sender: str = None,\n",
    "    recipient: str = None,\n",
    "    subject: str = None,\n",
    "    cc: bool = False,\n",
    "    labels: list[str] = None,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "    body: bool = False,\n",
    "    html: bool = False,\n",
    "    sort_by: str = \"date\",\n",
    "    sort_order: str = \"desc\",\n",
    "    limit: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This tool filter emails based on metadata such as sender (human), recipient (human), date range, or thread ID.\n",
    "    \n",
    "    Args:\n",
    "        uid (str, optional): Filter emails by their unique UID. Exact match required.\n",
    "        threadId: Filter emails by their conversation (email chian) thread ID, Returns all messages belonging to that specific chain (thread).\n",
    "        sender (str or list of str, optional): Filter emails by sender(s). Can be full email address, partial email, or sender names (case-insensitive, only humans).\n",
    "        recipient (str or list of str, optional): Filter emails by recipient(s). Can be full email addresses, partial emails, or recipient names, but strictly not numbers. (case-insensitive, only humans).\n",
    "        subject (str, optional): Filter email by subject text. Can be full or partial subject string (case-insensitive).\n",
    "        cc (bool, optional): Filter cc recepients of the email only when explicitly requested. Default False.\n",
    "        labels (list of str, optional): Filter emails by one or more labels. Matches any email that contains at least one of the provided labels (case-insensitive).\n",
    "        start_date (str, optional): Filter emails sent on or after this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        end_date (str, optional): Filter emails sent on or before this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        body (bool, optional): Include the plain-text email body only when explicitly requested. Default False.\n",
    "        html (bool, optional): Include the full HTML body only when explicitly requested. Default False.\n",
    "        sort_by (str, optional): Column to sort the results by. Default is 'date'.\n",
    "        sort_order (str, optional): Sort order: 'asc' for ascending, 'desc' for descending. Default is 'desc'.\n",
    "        limit (int, optional): Maximum number of results to return. Default is 5.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"email_filtering_tool is being called {uid}, {threadId}, {sender}, {recipient}, {subject}, {cc}, {labels}, {start_date}, {end_date}, {body}, {html}, {sort_by}, {sort_order}, {limit}\")\n",
    "    temp_df = df.clone()\n",
    "    mask = pl.lit(True)\n",
    "\n",
    "    if uid:\n",
    "        mask = mask & (pl.col(\"id\") == uid)\n",
    "\n",
    "    if threadId:\n",
    "        mask = mask & (pl.col(\"threadId\") == threadId)\n",
    "\n",
    "    # --- Sender filter (case-insensitive, matches name or email) ---\n",
    "    if sender:\n",
    "        sender = sender.lower()\n",
    "        # Add a normalized column\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "        ])\n",
    "        # Filter rows where the normalized 'from' matches sender\n",
    "        sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "        mask = mask & sender_mask\n",
    "\n",
    "    # --- Recipient filter ---\n",
    "    if recipient:\n",
    "        recipient = recipient.lower()\n",
    "        # Normalize 'to' and 'cc' columns which are lists\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "        ])\n",
    "        # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "        recipient_mask = (\n",
    "            pl.col(\"to_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "        )\n",
    "        if cc:\n",
    "            # Normalize 'to' and 'cc' columns which are lists\n",
    "            temp_df = temp_df.with_columns([\n",
    "                pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "            ])\n",
    "            # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "            cc_mask = (\n",
    "                pl.col(\"cc_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "            )\n",
    "            recipient_mask = recipient_mask | cc_mask\n",
    "\n",
    "        mask = mask & recipient_mask\n",
    "\n",
    "    # --- Date filtering (normalize to datetime) ---\n",
    "    if start_date or end_date:\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%SZ\", strict=False).alias(\"date_dt\")\n",
    "        ])\n",
    "        \n",
    "    if start_date:\n",
    "        try:\n",
    "            start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "            mask = mask & (pl.col(\"date_dt\") >= start_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing start_date: {e}\"\n",
    "\n",
    "    if end_date:\n",
    "        try:\n",
    "            end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1) - timedelta(seconds=1)\n",
    "            mask = mask & (pl.col(\"date_dt\") <= end_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing end_date: {e}\"\n",
    "        \n",
    "    if labels: \n",
    "        labels = [lbl.strip().lower() for lbl in labels]\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"lables\").map_elements(normalize_list, return_dtype=str).alias(\"labels_normalized\")\n",
    "        ])\n",
    "\n",
    "        labels_mask = pl.col(\"labels_normalized\").map_elements(\n",
    "            lambda email_lables: any(lbl in email_lables for lbl in labels),\n",
    "            return_dtype=bool\n",
    "        )\n",
    "\n",
    "        mask = mask & labels_mask\n",
    "\n",
    "    if subject:    \n",
    "        subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "        mask = mask & subject_mask\n",
    "\n",
    "    # Apply the mask only once\n",
    "    temp_df = temp_df.filter(mask)\n",
    "\n",
    "    # --- Sorting ---\n",
    "    temp_df = temp_df.sort(\n",
    "        by=sort_by,\n",
    "        descending=(sort_order.lower() == \"desc\")\n",
    "    )\n",
    "\n",
    "    # --- Handle empty result ---\n",
    "    if temp_df.is_empty():\n",
    "        return \"No emails found matching the specified criteria.\"\n",
    "\n",
    "    # --- Preview results ---\n",
    "    total_matches = temp_df.height\n",
    "    preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "    if body:\n",
    "        preview_cols.append(\"body\")\n",
    "    if html:\n",
    "        preview_cols.append(\"html\")\n",
    "\n",
    "    if limit is None:\n",
    "        results_preview = temp_df.select(preview_cols).to_dicts()\n",
    "    else:\n",
    "        results_preview = temp_df.head(limit).select(preview_cols).to_dicts()\n",
    "\n",
    "    def fmt(res):\n",
    "        parts = [\n",
    "            f\"id: {res.get('id','N/A')}\",\n",
    "            f\"ThreadId: {res.get('threadId','N/A')}\",\n",
    "            f\"From: {res.get('from','N/A')}\",\n",
    "            f\"To: {res.get('to','N/A')}\",\n",
    "            f\"CC: {res.get('cc','N/A')}\",\n",
    "            f\"Subject: {res.get('subject','N/A')}\",\n",
    "            f\"Date: {res.get('date','N/A')}\",\n",
    "            f\"Snippet: {res.get('snippet','N/A')}\",\n",
    "            f\"Labels: {res.get('labels','N/A')}\",\n",
    "            f\"Attachments: {res.get('attachments','N/A')}\",\n",
    "        ]\n",
    "        if body:\n",
    "            parts.append(f\"Body:\\n{res.get('body','N/A')}\")\n",
    "        if html:\n",
    "            parts.append(f\"HTML:\\n{res.get('html','N/A')}\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    formatted_results = \"\\n\\n---\\n\\n\".join(fmt(r) for r in results_preview)\n",
    "    shown = total_matches if limit is None else min(int(limit), total_matches)\n",
    "    return f\"Found {total_matches} emails matching the criteria. Showing {shown}:\\n\\n{formatted_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d47d2038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Re: Refund Statement - 805'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efed0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "from lib.load_data import df\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from lib.utils import normalize_list, match_value_in_columns, smart_subject_match\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert email summarizer.  \n",
    "\n",
    "Task:  \n",
    "- Input: Multiple emails with metadata (id, threadId, from, to, cc, Subject, date, snippet, body, labels, attachments).  \n",
    "- Group by ThreadId and summarize chronologically.  \n",
    "- Capture key points, actions, and important details with clarity and brevity.\n",
    "\n",
    "Summarize this,\n",
    "{chunk}\n",
    "\"\"\"\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_completion_tokens=512\n",
    ")\n",
    "\n",
    "encoding_model = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def get_chunks(text: str, chunk_size: int = 10000) -> List[str]:\n",
    "    \"\"\"Split a large text into token-based chunks.\"\"\"\n",
    "    tokens = encoding_model.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunk_text = encoding_model.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding_model.encode(text))\n",
    "\n",
    "def run_batch_task(tasks: List[Tuple[int, List[HumanMessage], int]], tpm_limit: int = 29000) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    tasks: list of (task_id, messages, est_tokens)\n",
    "    tpm_limit: max tokens/minute allowed\n",
    "    returns: list of (task_id, response_text)\n",
    "    \"\"\"\n",
    "    results: List[Tuple[int, str]] = []\n",
    "    current_batch: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    current_tokens = 0\n",
    "    window_start = time.time()\n",
    "\n",
    "    def flush(batch):\n",
    "        \"\"\"Send a batch to the LLM and record results.\"\"\"\n",
    "        nonlocal results\n",
    "        if not batch:\n",
    "            return\n",
    "        responses = llm.batch([msgs for _, msgs, _ in batch])\n",
    "        for (task_id, _, _), resp in zip(batch, responses):\n",
    "            results.append((task_id, resp.content))\n",
    "\n",
    "    for task in tasks:\n",
    "        _, _, tok = task\n",
    "\n",
    "        if current_tokens + tok > tpm_limit and current_batch:\n",
    "            flush(current_batch)\n",
    "            current_batch, current_tokens = [], 0\n",
    "\n",
    "            # respect TPM limit\n",
    "            elapsed = time.time() - window_start\n",
    "            if elapsed < 60:\n",
    "                time.sleep(60 - elapsed)\n",
    "            window_start = time.time()\n",
    "\n",
    "        current_batch.append(task)\n",
    "        current_tokens += tok\n",
    "\n",
    "    if current_batch:\n",
    "        flush(current_batch)\n",
    "\n",
    "    return results\n",
    " \n",
    "# @tool(\"email_filtering_tool\", parse_docstring=True)\n",
    "def email_filtering_tool(\n",
    "    uid: str = None,\n",
    "    threadId: str = None,\n",
    "    sender: str = None,\n",
    "    recipient: str = None,\n",
    "    subject: str = None,\n",
    "    cc: bool = False,\n",
    "    labels: list[str] = None,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "    body: bool = False,\n",
    "    html: bool = False,\n",
    "    sort_by: str = \"date\",\n",
    "    sort_order: str = \"desc\",\n",
    "    limit: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This tool filter emails based on metadata such as sender (human), recipient (human), date range, or thread ID.\n",
    "    \n",
    "    Args:\n",
    "        uid (str, optional): Filter emails by their unique UID. Exact match required.\n",
    "        threadId: Filter emails by their conversation (email chian) thread ID, Returns all messages belonging to that specific chain (thread).\n",
    "        sender (str or list of str, optional): Filter emails by sender(s). Can be full email address, partial email, or sender names (case-insensitive, only humans).\n",
    "        recipient (str or list of str, optional): Filter emails by recipient(s). Can be full email addresses, partial emails, or recipient names, but strictly not numbers. (case-insensitive, only humans).\n",
    "        subject (str, optional): Filter email by subject text. Can be full or partial subject string (case-insensitive).\n",
    "        cc (bool, optional): Filter cc recepients of the email only when explicitly requested. Default False.\n",
    "        labels (list of str, optional): Filter emails by one or more labels. Matches any email that contains at least one of the provided labels (case-insensitive).\n",
    "        start_date (str, optional): Filter emails sent on or after this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        end_date (str, optional): Filter emails sent on or before this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        body (bool, optional): Include the plain-text email body only when explicitly requested. Default False.\n",
    "        html (bool, optional): Include the full HTML body only when explicitly requested. Default False.\n",
    "        sort_by (str, optional): Column to sort the results by. Default is 'date'.\n",
    "        sort_order (str, optional): Sort order: 'asc' for ascending, 'desc' for descending. Default is 'desc'.\n",
    "        limit (int, optional): Maximum number of results to return. Default is 5.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"email_filtering_tool is being called {uid}, {threadId}, {sender}, {recipient}, {subject}, {cc}, {labels}, {start_date}, {end_date}, {body}, {html}, {sort_by}, {sort_order}, {limit}\")\n",
    "    temp_df = df.clone()\n",
    "    mask = pl.lit(True)\n",
    "\n",
    "    temp_df = temp_df.with_columns([\n",
    "        temp_df[\"body\"].struct.field(\"text\").alias(\"body_text\"),\n",
    "        temp_df[\"body\"].struct.field(\"html\").alias(\"body_html\"),\n",
    "    ])\n",
    "\n",
    "    if uid:\n",
    "        mask = mask & (pl.col(\"id\") == uid)\n",
    "\n",
    "    if threadId:\n",
    "        mask = mask & (pl.col(\"threadId\") == threadId)\n",
    "\n",
    "    # --- Sender filter (case-insensitive, matches name or email) ---\n",
    "    if sender:\n",
    "        sender = sender.lower()\n",
    "        # Add a normalized column\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "        ])\n",
    "        # Filter rows where the normalized 'from' matches sender\n",
    "        sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "        mask = mask & sender_mask\n",
    "\n",
    "    # --- Recipient filter ---\n",
    "    if recipient:\n",
    "        recipient = recipient.lower()\n",
    "        # Normalize 'to' and 'cc' columns which are lists\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "        ])\n",
    "        # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "        recipient_mask = (\n",
    "            pl.col(\"to_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "        )\n",
    "        if cc:\n",
    "            # Normalize 'to' and 'cc' columns which are lists\n",
    "            temp_df = temp_df.with_columns([\n",
    "                pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "            ])\n",
    "            # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "            cc_mask = (\n",
    "                pl.col(\"cc_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "            )\n",
    "            recipient_mask = recipient_mask | cc_mask\n",
    "\n",
    "        mask = mask & recipient_mask\n",
    "\n",
    "    # --- Date filtering (normalize to datetime) ---\n",
    "    if start_date or end_date:\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%SZ\", strict=False).alias(\"date_dt\")\n",
    "        ])\n",
    "        \n",
    "    if start_date:\n",
    "        try:\n",
    "            start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "            mask = mask & (pl.col(\"date_dt\") >= start_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing start_date: {e}\"\n",
    "\n",
    "    if end_date:\n",
    "        try:\n",
    "            end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1) - timedelta(seconds=1)\n",
    "            mask = mask & (pl.col(\"date_dt\") <= end_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing end_date: {e}\"\n",
    "        \n",
    "    if labels: \n",
    "        labels = [lbl.strip().lower() for lbl in labels]\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"lables\").map_elements(normalize_list, return_dtype=str).alias(\"labels_normalized\")\n",
    "        ])\n",
    "\n",
    "        labels_mask = pl.col(\"labels_normalized\").map_elements(\n",
    "            lambda email_lables: any(lbl in email_lables for lbl in labels),\n",
    "            return_dtype=bool\n",
    "        )\n",
    "\n",
    "        mask = mask & labels_mask\n",
    "\n",
    "    if subject:    \n",
    "        subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "        mask = mask & subject_mask\n",
    "\n",
    "    # Apply the mask only once\n",
    "    temp_df = temp_df.filter(mask)\n",
    "\n",
    "    # --- Sorting ---\n",
    "    temp_df = temp_df.sort(\n",
    "        by=sort_by,\n",
    "        descending=(sort_order.lower() == \"desc\")\n",
    "    )\n",
    "\n",
    "    # --- Handle empty result ---\n",
    "    if temp_df.is_empty():\n",
    "        return \"No emails found matching the specified criteria.\"\n",
    "\n",
    "    # --- Preview results ---\n",
    "    total_matches = temp_df.height\n",
    "    preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "    if body:\n",
    "        preview_cols.append(\"body_text\")\n",
    "    if html:\n",
    "        preview_cols.append(\"body_html\")\n",
    "\n",
    "    if limit is None:\n",
    "        results_preview = temp_df.select(preview_cols).to_dicts()\n",
    "    else:\n",
    "        results_preview = temp_df.head(limit).select(preview_cols).to_dicts()\n",
    "\n",
    "    def fmt(res):\n",
    "        parts = [\n",
    "            f\"id: {res.get('id','N/A')}\",\n",
    "            f\"ThreadId: {res.get('threadId','N/A')}\",\n",
    "            f\"From: {res.get('from','N/A')}\",\n",
    "            f\"To: {res.get('to','N/A')}\",\n",
    "            f\"CC: {res.get('cc','N/A')}\",\n",
    "            f\"Subject: {res.get('subject','N/A')}\",\n",
    "            f\"Date: {res.get('date','N/A')}\",\n",
    "            f\"Snippet: {res.get('snippet','N/A')}\",\n",
    "            f\"Labels: {res.get('labels','N/A')}\",\n",
    "            f\"Attachments: {res.get('attachments','N/A')}\",\n",
    "        ]\n",
    "        if body:\n",
    "            parts.append(f\"Body: {res.get('body_text','N/A')}\")\n",
    "        if html:\n",
    "            parts.append(f\"HTML: {res.get('body_html','N/A')}\")\n",
    "        return \"\\n\".join(parts)\n",
    "    \n",
    "    tasks:List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    def hierarchical_summary(formatted_results: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Summarize results hierarchically, safely splitting large emails by token count.\n",
    "        \"\"\"\n",
    "        for i, email_text in enumerate(formatted_results):\n",
    "            tokens_est = count_tokens(email_text)\n",
    "            formatted_prompt = prompt_perspective.format(chunk=email_text)\n",
    "            tasks.append((i, [HumanMessage(content=formatted_prompt)], tokens_est))\n",
    "\n",
    "    # formatted_results = \"\\n\\n---\\n\\n\".join(fmt(r) for r in results_preview)\n",
    "    formatted_results = [fmt(r) for r in results_preview]\n",
    "    tasks = hierarchical_summary(formatted_results)\n",
    "    results = run_batch_task(tasks)\n",
    "\n",
    "    shown = total_matches if limit is None else min(int(limit), total_matches)\n",
    "    return f\"Found {total_matches} emails matching the criteria. Showing {shown}:\\n\\n{formatted_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c0d4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_filtering_tool is being called None, None, Nirav Joshi, Sankar Narayan, Refund Statement 805, False, None, None, None, True, False, date, desc, 3\n",
      "1610\n",
      "--------------\n",
      "1444\n",
      "--------------\n",
      "1343\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Found 3 emails matching the criteria. Showing 3:\\n\\n[\"id: 199098a3358031c1\\\\nThreadId: 198f02e745824fb8\\\\nFrom: Nirav Joshi <nirav.j05@gmail.com>\\\\nTo: [\\'sankar.narayanan@2getherments.com\\']\\\\nCC: [\\'hari@2getherments.com\\', \\'pallavi@2getherments.com\\', \\'meena@2getherments.com\\', \\'customer.communications@2getherments.com\\', \\'ramakrishna@2getherments.com\\', \\'shachiraju@2getherments.com\\', \\'arshkaur19@gmail.com\\']\\\\nSubject: Re: Refund Statement - 805\\\\nDate: 2025-09-02T13:57:42+05:30Z\\\\nSnippet: Hi Rama sir, Please update status on this request. Thank you. On Thu, Aug 28, 2025 at 5:20 PM Sankar Narayanan &lt;sankar.narayanan@2getherments.com&gt; wrote: @Rama Krishna Please proceed further. On\\\\nLabels: [\\'IMPORTANT\\', \\'CATEGORY_PERSONAL\\', \\'INBOX\\']\\\\nAttachments: []\\\\nBody: Hi Rama sir,\\\\r\\\\n\\\\r\\\\nPlease update status on this request.\\\\r\\\\nThank you.\\\\r\\\\n\\\\r\\\\nOn Thu, Aug 28, 2025 at 5:20\\\\u202fPM Sankar Narayanan <\\\\r\\\\nsankar.narayanan@2getherments.com> wrote:\\\\r\\\\n\\\\r\\\\n> @Rama Krishna <ramakrishna@2getherments.com>  Please proceed further.\\\\r\\\\n>\\\\r\\\\n> On Thu, Aug 28, 2025 at 5:05\\\\u202fPM Nirav Joshi <nirav.j05@gmail.com> wrote:\\\\r\\\\n>\\\\r\\\\n>> Thank you Sankar sir for your swift response.\\\\r\\\\n>>\\\\r\\\\n>> Please proceed with the same.\\\\r\\\\n>>\\\\r\\\\n>> On Thu, Aug 28, 2025 at 4:07\\\\u202fPM Nirav Joshi <nirav.j05@gmail.com> wrote:\\\\r\\\\n>>\\\\r\\\\n>>> Dear Sankar sir,\\\\r\\\\n>>>\\\\r\\\\n>>> Can you please share the Excel file and there is Total missing in above\\\\r\\\\n>>> email can you please make corrections and share with us.\\\\r\\\\n>>>\\\\r\\\\n>>> On Thu, Aug 28, 2025 at 3:47\\\\u202fPM Sankar Narayanan <\\\\r\\\\n>>> sankar.narayanan@2getherments.com> wrote:\\\\r\\\\n>>>\\\\r\\\\n>>>> Dear Nirav,\\\\r\\\\n>>>> As discussed,Please find the refund statement attached for\\\\r\\\\n>>>> your reference.\\\\r\\\\n>>>> Kindly Acknowledge to proceed further.\\\\r\\\\n>>>>\\\\r\\\\n>>>> Shankar.S\\\\r\\\\n>>>> Manager-Customer Relations\\\\r\\\\n>>>> Ph:63669 55995\\\\r\\\\n>>>>\\\\r\\\\n>>>>\\\\r\\\\n>>>>   805 Refund Statement\\\\r\\\\n>>>> S.NO DESCRIPTION  UNITS NO\\'S QTY Rate Bill amount\\\\r\\\\n>>>>\\\\r\\\\n>>>>             FLOOR TILE\\\\r\\\\n>>>> 1 FOYER, LIVING,Guest,POOJA, KITCHEN & DINING SQ.F 1 504.00 42.00\\\\r\\\\n>>>> 21168.00\\\\r\\\\n>>>> 2 M.BED ROOM SQ.F 1 151.00 42.00 6342.00\\\\r\\\\n>>>> 3 K.BED ROOM SQ.F 1 152.00 42.00 6384.00\\\\r\\\\n>>>> 4 Balcony Tiles SQ.F   80.00 38.00 3040.00\\\\r\\\\n>>>>   TOTAL SQ.FT SQ.F   887.00\\\\r\\\\n>>>>   Total QTY in SFT     887.00   36934.00\\\\r\\\\n>>>> 4 Skirting RFT   98.00 42.00 4116.00\\\\r\\\\n>>>> FLOOR TILE\\\\r\\\\n>>>> 5 M.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>>>> 6 C.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>>>> 7 KIDS TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>>>> 8 UTILITY SQ.F 1 50.00 40.00 2000.00\\\\r\\\\n>>>>   TOTAL SQ.FT     170.00   6800.00\\\\r\\\\n>>>>              Wall TILE\\\\r\\\\n>>>> 10 M.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>>>> 11 C.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>>>> 12 KIDS TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>>>> 13 UTILITY SQ.F 1 115.00 30.00 3450.00\\\\r\\\\n>>>> 14 BALCONY SQ.F 1 28.00 42.00 1176.00\\\\r\\\\n>>>> 15 Kitchen Wall Tile dado SQ.F 1 20.00 45.00 900.00\\\\r\\\\n>>>> 16 TOTAL  SQ.F 1 580.00\\\\r\\\\n>>>> 17 TOTAL SQ.FT SQ.F   580.00   24291.00\\\\r\\\\n>>>> 18 TOTAL Floor area SQ.F 1     43734.00\\\\r\\\\n>>>> 19 TOTAL wall tile area SQ.F 1     24291.00\\\\r\\\\n>>>> 20 Skirting Qty SQ.F 1     4116.00\\\\r\\\\n>>>> 21 Kitchen Granite  SQ.F       1080.00\\\\r\\\\n>>>> 22 Commorde    1     8140.00\\\\r\\\\n>>>> 23 Shower    1     5640.00\\\\r\\\\n>>>> 24 wash basin    1     43332.00\\\\r\\\\n>>>> 25 Second Coat Painting          17332.00\\\\r\\\\n>>>> 26 Internal Door Finishes         10300.00\\\\r\\\\n>>>> 27 Wash Basin (Including all fittings & Installations)     4.00 4200\\\\r\\\\n>>>> 16800.00\\\\r\\\\n>>>> 28 Kitchen Sink (Including all fittings & Installations)\\\\r\\\\n>>>> 4300.00\\\\r\\\\n>>>> 29 Grid Ceiling (2 Toilets)         6500.00\\\\r\\\\n>>>> 30 Safety Grills         15000.00\\\\r\\\\n>>>> 31 Main Door Polish         2500.00\\\\r\\\\n>>>> 32 Internal  Door frames (Bedrooms)   3 7100.00   21300.00\\\\r\\\\n>>>> 33 Toilet  Door frames   3 5800.00   17400.00\\\\r\\\\n>>>> 34 Internal  Door   3 2500.00   7500.00\\\\r\\\\n>>>>\\\\r\\\\n>>>>   Total Amount Refundable      249265\\\\r\\\\n>>>> 805- Additional Modifications\\\\r\\\\n>>>> S.No Description Rates\\\\r\\\\n>>>> 1 Core Cutting in utility 1500\\\\r\\\\n>>>> 2 Plumbing extra points in utility 2500\\\\r\\\\n>>>> 3 2 Bathroom Extra Tiles laying  with material 14000\\\\r\\\\n>>>> 4 Full Flat Tiles Laying For Design Tiles & Bigger tiles 5000\\\\r\\\\n>>>> 5 Divertor Fixing Labour Charges 2600\\\\r\\\\n>>>> 6 Full Flat Epoxy Extra Labour Charges 2500\\\\r\\\\n>>>> 7 Utility Concrete Beds For Both Sides(100mm) 4500\\\\r\\\\n>>>> 8 Electrical extra point in utility 1200\\\\r\\\\n>>>>   Total Modifications Amount 33800\\\\r\\\\n>>>> After Deduction 215465\\\\r\\\\n>>>>\\\\r\\\\n>>>>\\\\r\\\\n>>>\\\\r\\\\n>>> --\\\\r\\\\n>>> Thanks & Regards\\\\r\\\\n>>> Nirav H Joshi\\\\r\\\\n>>> 91-9740077339\\\\r\\\\n>>>\\\\r\\\\n>>\\\\r\\\\n>>\\\\r\\\\n>> --\\\\r\\\\n>> Thanks & Regards\\\\r\\\\n>> Nirav H Joshi\\\\r\\\\n>> 91-9740077339\\\\r\\\\n>>\\\\r\\\\n>\\\\r\\\\n\\\\r\\\\n-- \\\\r\\\\nThanks & Regards\\\\r\\\\nNirav H Joshi\\\\r\\\\n91-9740077339\\\\r\\\\n\", \"id: 198f075c6bc02c3c\\\\nThreadId: 198f02e745824fb8\\\\nFrom: Nirav Joshi <nirav.j05@gmail.com>\\\\nTo: [\\'sankar.narayanan@2getherments.com\\']\\\\nCC: [\\'hari@2getherments.com\\', \\'pallavi@2getherments.com\\', \\'meena@2getherments.com\\', \\'customer.communications@2getherments.com\\', \\'ramakrishna@2getherments.com\\', \\'shachiraju@2getherments.com\\']\\\\nSubject: Re: Refund Statement - 805\\\\nDate: 2025-08-28T17:04:52+05:30Z\\\\nSnippet: Thank you Sankar sir for your swift response. Please proceed with the same. On Thu, Aug 28, 2025 at 4:07 PM Nirav Joshi &lt;nirav.j05@gmail.com&gt; wrote: Dear Sankar sir, Can you please share the\\\\nLabels: [\\'UNREAD\\', \\'IMPORTANT\\', \\'CATEGORY_PERSONAL\\', \\'INBOX\\']\\\\nAttachments: []\\\\nBody: Thank you Sankar sir for your swift response.\\\\r\\\\n\\\\r\\\\nPlease proceed with the same.\\\\r\\\\n\\\\r\\\\nOn Thu, Aug 28, 2025 at 4:07\\\\u202fPM Nirav Joshi <nirav.j05@gmail.com> wrote:\\\\r\\\\n\\\\r\\\\n> Dear Sankar sir,\\\\r\\\\n>\\\\r\\\\n> Can you please share the Excel file and there is Total missing in above\\\\r\\\\n> email can you please make corrections and share with us.\\\\r\\\\n>\\\\r\\\\n> On Thu, Aug 28, 2025 at 3:47\\\\u202fPM Sankar Narayanan <\\\\r\\\\n> sankar.narayanan@2getherments.com> wrote:\\\\r\\\\n>\\\\r\\\\n>> Dear Nirav,\\\\r\\\\n>> As discussed,Please find the refund statement attached for your reference.\\\\r\\\\n>> Kindly Acknowledge to proceed further.\\\\r\\\\n>>\\\\r\\\\n>> Shankar.S\\\\r\\\\n>> Manager-Customer Relations\\\\r\\\\n>> Ph:63669 55995\\\\r\\\\n>>\\\\r\\\\n>>\\\\r\\\\n>>   805 Refund Statement\\\\r\\\\n>> S.NO DESCRIPTION  UNITS NO\\'S QTY Rate Bill amount\\\\r\\\\n>>\\\\r\\\\n>>             FLOOR TILE\\\\r\\\\n>> 1 FOYER, LIVING,Guest,POOJA, KITCHEN & DINING SQ.F 1 504.00 42.00\\\\r\\\\n>> 21168.00\\\\r\\\\n>> 2 M.BED ROOM SQ.F 1 151.00 42.00 6342.00\\\\r\\\\n>> 3 K.BED ROOM SQ.F 1 152.00 42.00 6384.00\\\\r\\\\n>> 4 Balcony Tiles SQ.F   80.00 38.00 3040.00\\\\r\\\\n>>   TOTAL SQ.FT SQ.F   887.00\\\\r\\\\n>>   Total QTY in SFT     887.00   36934.00\\\\r\\\\n>> 4 Skirting RFT   98.00 42.00 4116.00\\\\r\\\\n>> FLOOR TILE\\\\r\\\\n>> 5 M.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>> 6 C.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>> 7 KIDS TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n>> 8 UTILITY SQ.F 1 50.00 40.00 2000.00\\\\r\\\\n>>   TOTAL SQ.FT     170.00   6800.00\\\\r\\\\n>>              Wall TILE\\\\r\\\\n>> 10 M.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>> 11 C.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>> 12 KIDS TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n>> 13 UTILITY SQ.F 1 115.00 30.00 3450.00\\\\r\\\\n>> 14 BALCONY SQ.F 1 28.00 42.00 1176.00\\\\r\\\\n>> 15 Kitchen Wall Tile dado SQ.F 1 20.00 45.00 900.00\\\\r\\\\n>> 16 TOTAL  SQ.F 1 580.00\\\\r\\\\n>> 17 TOTAL SQ.FT SQ.F   580.00   24291.00\\\\r\\\\n>> 18 TOTAL Floor area SQ.F 1     43734.00\\\\r\\\\n>> 19 TOTAL wall tile area SQ.F 1     24291.00\\\\r\\\\n>> 20 Skirting Qty SQ.F 1     4116.00\\\\r\\\\n>> 21 Kitchen Granite  SQ.F       1080.00\\\\r\\\\n>> 22 Commorde    1     8140.00\\\\r\\\\n>> 23 Shower    1     5640.00\\\\r\\\\n>> 24 wash basin    1     43332.00\\\\r\\\\n>> 25 Second Coat Painting          17332.00\\\\r\\\\n>> 26 Internal Door Finishes         10300.00\\\\r\\\\n>> 27 Wash Basin (Including all fittings & Installations)     4.00 4200\\\\r\\\\n>> 16800.00\\\\r\\\\n>> 28 Kitchen Sink (Including all fittings & Installations)         4300.00\\\\r\\\\n>> 29 Grid Ceiling (2 Toilets)         6500.00\\\\r\\\\n>> 30 Safety Grills         15000.00\\\\r\\\\n>> 31 Main Door Polish         2500.00\\\\r\\\\n>> 32 Internal  Door frames (Bedrooms)   3 7100.00   21300.00\\\\r\\\\n>> 33 Toilet  Door frames   3 5800.00   17400.00\\\\r\\\\n>> 34 Internal  Door   3 2500.00   7500.00\\\\r\\\\n>>\\\\r\\\\n>>   Total Amount Refundable      249265\\\\r\\\\n>> 805- Additional Modifications\\\\r\\\\n>> S.No Description Rates\\\\r\\\\n>> 1 Core Cutting in utility 1500\\\\r\\\\n>> 2 Plumbing extra points in utility 2500\\\\r\\\\n>> 3 2 Bathroom Extra Tiles laying  with material 14000\\\\r\\\\n>> 4 Full Flat Tiles Laying For Design Tiles & Bigger tiles 5000\\\\r\\\\n>> 5 Divertor Fixing Labour Charges 2600\\\\r\\\\n>> 6 Full Flat Epoxy Extra Labour Charges 2500\\\\r\\\\n>> 7 Utility Concrete Beds For Both Sides(100mm) 4500\\\\r\\\\n>> 8 Electrical extra point in utility 1200\\\\r\\\\n>>   Total Modifications Amount 33800\\\\r\\\\n>> After Deduction 215465\\\\r\\\\n>>\\\\r\\\\n>>\\\\r\\\\n>\\\\r\\\\n> --\\\\r\\\\n> Thanks & Regards\\\\r\\\\n> Nirav H Joshi\\\\r\\\\n> 91-9740077339\\\\r\\\\n>\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n-- \\\\r\\\\nThanks & Regards\\\\r\\\\nNirav H Joshi\\\\r\\\\n91-9740077339\\\\r\\\\n\", \"id: 198f0418716eeeef\\\\nThreadId: 198f02e745824fb8\\\\nFrom: Nirav Joshi <nirav.j05@gmail.com>\\\\nTo: [\\'sankar.narayanan@2getherments.com\\']\\\\nCC: [\\'hari@2getherments.com\\', \\'pallavi@2getherments.com\\', \\'meena@2getherments.com\\', \\'customer.communications@2getherments.com\\', \\'ramakrishna@2getherments.com\\', \\'shachiraju@2getherments.com\\']\\\\nSubject: Re: Refund Statement - 805\\\\nDate: 2025-08-28T16:07:48+05:30Z\\\\nSnippet: Dear Sankar sir, Can you please share the Excel file and there is Total missing in above email can you please make corrections and share with us. On Thu, Aug 28, 2025 at 3:47 PM Sankar Narayanan &lt;\\\\nLabels: [\\'UNREAD\\', \\'IMPORTANT\\', \\'CATEGORY_PERSONAL\\', \\'INBOX\\']\\\\nAttachments: []\\\\nBody: Dear Sankar sir,\\\\r\\\\n\\\\r\\\\nCan you please share the Excel file and there is Total missing in above\\\\r\\\\nemail can you please make corrections and share with us.\\\\r\\\\n\\\\r\\\\nOn Thu, Aug 28, 2025 at 3:47\\\\u202fPM Sankar Narayanan <\\\\r\\\\nsankar.narayanan@2getherments.com> wrote:\\\\r\\\\n\\\\r\\\\n> Dear Nirav,\\\\r\\\\n> As discussed,Please find the refund statement attached for your reference.\\\\r\\\\n> Kindly Acknowledge to proceed further.\\\\r\\\\n>\\\\r\\\\n> Shankar.S\\\\r\\\\n> Manager-Customer Relations\\\\r\\\\n> Ph:63669 55995\\\\r\\\\n>\\\\r\\\\n>\\\\r\\\\n>   805 Refund Statement\\\\r\\\\n> S.NO DESCRIPTION  UNITS NO\\'S QTY Rate Bill amount\\\\r\\\\n>\\\\r\\\\n>             FLOOR TILE\\\\r\\\\n> 1 FOYER, LIVING,Guest,POOJA, KITCHEN & DINING SQ.F 1 504.00 42.00 21168.00\\\\r\\\\n> 2 M.BED ROOM SQ.F 1 151.00 42.00 6342.00\\\\r\\\\n> 3 K.BED ROOM SQ.F 1 152.00 42.00 6384.00\\\\r\\\\n> 4 Balcony Tiles SQ.F   80.00 38.00 3040.00\\\\r\\\\n>   TOTAL SQ.FT SQ.F   887.00\\\\r\\\\n>   Total QTY in SFT     887.00   36934.00\\\\r\\\\n> 4 Skirting RFT   98.00 42.00 4116.00\\\\r\\\\n> FLOOR TILE\\\\r\\\\n> 5 M.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n> 6 C.TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n> 7 KIDS TOILET SQ.F 1 40.00 40.00 1600.00\\\\r\\\\n> 8 UTILITY SQ.F 1 50.00 40.00 2000.00\\\\r\\\\n>   TOTAL SQ.FT     170.00   6800.00\\\\r\\\\n>              Wall TILE\\\\r\\\\n> 10 M.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n> 11 C.TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n> 12 KIDS TOILET SQ.F 1 139.00 45.00 6255.00\\\\r\\\\n> 13 UTILITY SQ.F 1 115.00 30.00 3450.00\\\\r\\\\n> 14 BALCONY SQ.F 1 28.00 42.00 1176.00\\\\r\\\\n> 15 Kitchen Wall Tile dado SQ.F 1 20.00 45.00 900.00\\\\r\\\\n> 16 TOTAL  SQ.F 1 580.00\\\\r\\\\n> 17 TOTAL SQ.FT SQ.F   580.00   24291.00\\\\r\\\\n> 18 TOTAL Floor area SQ.F 1     43734.00\\\\r\\\\n> 19 TOTAL wall tile area SQ.F 1     24291.00\\\\r\\\\n> 20 Skirting Qty SQ.F 1     4116.00\\\\r\\\\n> 21 Kitchen Granite  SQ.F       1080.00\\\\r\\\\n> 22 Commorde    1     8140.00\\\\r\\\\n> 23 Shower    1     5640.00\\\\r\\\\n> 24 wash basin    1     43332.00\\\\r\\\\n> 25 Second Coat Painting          17332.00\\\\r\\\\n> 26 Internal Door Finishes         10300.00\\\\r\\\\n> 27 Wash Basin (Including all fittings & Installations)     4.00 4200\\\\r\\\\n> 16800.00\\\\r\\\\n> 28 Kitchen Sink (Including all fittings & Installations)         4300.00\\\\r\\\\n> 29 Grid Ceiling (2 Toilets)         6500.00\\\\r\\\\n> 30 Safety Grills         15000.00\\\\r\\\\n> 31 Main Door Polish         2500.00\\\\r\\\\n> 32 Internal  Door frames (Bedrooms)   3 7100.00   21300.00\\\\r\\\\n> 33 Toilet  Door frames   3 5800.00   17400.00\\\\r\\\\n> 34 Internal  Door   3 2500.00   7500.00\\\\r\\\\n>\\\\r\\\\n>   Total Amount Refundable      249265\\\\r\\\\n> 805- Additional Modifications\\\\r\\\\n> S.No Description Rates\\\\r\\\\n> 1 Core Cutting in utility 1500\\\\r\\\\n> 2 Plumbing extra points in utility 2500\\\\r\\\\n> 3 2 Bathroom Extra Tiles laying  with material 14000\\\\r\\\\n> 4 Full Flat Tiles Laying For Design Tiles & Bigger tiles 5000\\\\r\\\\n> 5 Divertor Fixing Labour Charges 2600\\\\r\\\\n> 6 Full Flat Epoxy Extra Labour Charges 2500\\\\r\\\\n> 7 Utility Concrete Beds For Both Sides(100mm) 4500\\\\r\\\\n> 8 Electrical extra point in utility 1200\\\\r\\\\n>   Total Modifications Amount 33800\\\\r\\\\n> After Deduction 215465\\\\r\\\\n>\\\\r\\\\n>\\\\r\\\\n\\\\r\\\\n-- \\\\r\\\\nThanks & Regards\\\\r\\\\nNirav H Joshi\\\\r\\\\n91-9740077339\\\\r\\\\n\"]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_filtering_tool(\n",
    "    uid=None,\n",
    "    threadId=None,\n",
    "    sender=\"Nirav Joshi\",\n",
    "    recipient=\"Sankar Narayan\",\n",
    "    subject=\"Refund Statement 805\",\n",
    "    cc=False,\n",
    "    labels=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    body=True,\n",
    "    html=False,\n",
    "    sort_by=\"date\",\n",
    "    sort_order=\"desc\",\n",
    "    limit=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
