{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed11fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"d:\\VSCode\\re-assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a46655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command-line environment detected. Using local data file.\n",
      "Loading email metadata from: d:\\VSCode\\re-assistant\\lib\\data\\all_mails.jsonl\n",
      "Successfully loaded 11807 records for metadata.\n",
      "Connecting to ChromaDB Vector Store...\n",
      "Successfully connected to ChromaDB collection.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "import redis\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from lib.utils import AGENT_MODEL, SYSTEM_PROMPT\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from lib.db.db_service import ThreadService\n",
    "# from lib.db.db_conn import conn\n",
    "from datetime import datetime\n",
    "from lib.load_data import df\n",
    "from rapidfuzz import fuzz\n",
    "from lib.utils import match_value_in_columns, normalize_email_field, normalize_list\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d020899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import polars as pl\n",
    "from langchain.tools import tool\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.clone()\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "])\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "])\n",
    "\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "])\n",
    "\n",
    "print(temp_df['from_normalized'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da609c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def build_name_dict(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorized, memory-efficient building of:\n",
    "        token -> {\"full\": full_name, \"emails\": [list of emails]}\n",
    "\n",
    "    Uses DataFrame.unpivot (replacement for deprecated melt), explode, and Polars string ops.\n",
    "    \"\"\"\n",
    "    cols = [c for c in [\"from_normalized\", \"to_normalized\", \"cc_normalized\"] if c in df.columns]\n",
    "    if not cols:\n",
    "        raise ValueError(\"No normalized columns found. Expect one of: from_normalized, to_normalized, cc_normalized\")\n",
    "\n",
    "    # 1) unpivot (stack the normalized columns into a single column \"addr\")\n",
    "    stacked = df.unpivot(index=[], on=cols, variable_name=\"src\", value_name=\"addr\")\n",
    "\n",
    "    stacked = stacked.filter(pl.col(\"addr\") != \"\")\n",
    "\n",
    "    stacked = stacked.with_columns(\n",
    "        pl.col(\"addr\").str.split(\",\").alias(\"addr_list\")\n",
    "    )\n",
    "\n",
    "    stacked = stacked.explode(\"addr_list\")\n",
    "\n",
    "    stacked = stacked.with_columns(\n",
    "        pl.col(\"addr_list\").str.strip_chars().alias(\"addr\")\n",
    "    ).drop(\"addr_list\")\n",
    "\n",
    "    stacked = stacked.filter(\n",
    "        pl.col(\"addr\").is_first_distinct().alias(\"unique_addr\")\n",
    "    )\n",
    "    \n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d13977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = build_name_dict(temp_df)\n",
    "names_series = stacked['addr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acaf7700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2_646,)\n",
      "Series: 'addr' [str]\n",
      "[\n",
      "\t\"harish.sales harish.sales@ahlaâ€¦\n",
      "\t\"contact 2getherments infra pvtâ€¦\n",
      "\t\"balakrishna info@2getherments.â€¦\n",
      "\t\"malini satish kumar malini.satâ€¦\n",
      "\t\"customer communications 2g cx â€¦\n",
      "\tâ€¦\n",
      "\t\"neeti1919@gmail.com\"\n",
      "\t\"nwajit@gmail.com\"\n",
      "\t\"arshkaur19@gmail.com\"\n",
      "\t\"anjalisinha373@gmail.com\"\n",
      "\t\"anindita92nayak@gmail.com\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(names_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a91a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json(raw_response):\n",
    "    if not raw_response:\n",
    "        return None\n",
    "    match = re.search(r'\\{.*\\}', raw_response, re.S)\n",
    "    if match:\n",
    "        return json.loads(match.group(0))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de57a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176b9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0, max_tokens=300, api_key='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')\n",
    "\n",
    "def run_batch_task(tasks: List[Tuple[int, List[HumanMessage], int]], tpm_limit: int = 180000) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    tasks: list of (task_id, messages, est_tokens)\n",
    "    tpm_limit: max tokens/minute allowed\n",
    "    returns: list of (task_id, response_text)\n",
    "    \"\"\"\n",
    "    results: List[Tuple[int, str]] = []\n",
    "    current_batch: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    current_tokens = 0\n",
    "    window_start = time.time()\n",
    "\n",
    "    def flush(batch):\n",
    "      \"\"\"Send a batch to the LLM and record results.\"\"\"\n",
    "      print(f\"\\nðŸš€ Flushing {len(batch)} tasks \"\n",
    "              f\"({sum(tok for _, _, tok in batch)} tokens)...\")\n",
    "\n",
    "      responses = llm.batch([msgs for _, msgs, _ in batch])\n",
    "      for (task_id, _, _), resp in zip(batch, responses):\n",
    "        print(f\"   âœ… Task {task_id} completed.\")\n",
    "        results.append((task_id, resp.content))\n",
    "\n",
    "    for task in tasks:\n",
    "      _, _, tok = task\n",
    "\n",
    "      if current_tokens + tok > tpm_limit and current_batch:\n",
    "        flush(current_batch)\n",
    "        current_batch, current_tokens = [], 0\n",
    "\n",
    "        elapsed = time.time() - window_start\n",
    "        if elapsed < 60:\n",
    "          time.sleep(60 - elapsed)\n",
    "        window_start = time.time()\n",
    "\n",
    "      current_batch.append(task)\n",
    "      current_tokens += tok\n",
    "    \n",
    "    if current_batch:\n",
    "      flush(current_batch)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c64f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from langchain.schema  import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an information extraction system.  \n",
    "\n",
    "Task:  \n",
    "- Extract only meaningful PERSON NAME (not initials like 1-2 letters) tokens from the given string.\n",
    "- Extract **every EMAIL address as a token without exception.**\n",
    "- Emails must always be included, even if they contain numbers, company names, or other patterns.\n",
    "- Split multi-part names into separate tokens (e.g., \"Satish Kumar s\" â†’ [\"satish\", \"kumar\"]).\n",
    "- Ignore company suffixes or terms (e.g., \"Ltd\", \"Inc\", \"Pvt\", \"Sales\"), standalone numbers, and generic stop-words.\n",
    "- Convert all tokens to lowercase.\n",
    "- Return the result as a **strict JSON object only**, with no explanations, extra text, or formatting.\n",
    "- Always return in the exact format below, with valid JSON only (no trailing commas, no comments).\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "  \"tokens\": [\"\", \"\"]\n",
    "}}\n",
    "\n",
    "Examples:  \n",
    "Input: \"customer cx customer.communications@2getherments.com\"  \n",
    "Output: {{ \"tokens\": [\"customer\", \"customer.communications@2getherments.com\"] }}\n",
    "\n",
    "Input: \"213 rahul sinha rahulsinha198@gmail.com\"  \n",
    "Output: {{ \"tokens\": [\"rahul\", \"sinha\", \"rahulsinha198@gmail.com\"] }}\n",
    "\n",
    "Input: \"pavan hs hspavankumar@yahoo.com\"  \n",
    "Output: {{ \"tokens\": [\"pavan\", \"hspavankumar@yahoo.com\"] }}\n",
    "\n",
    "Now process this input:  \n",
    "Full string: {full_name}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "tasks:List[Tuple[int, List[HumanMessage], int]] = []\n",
    "\n",
    "for idx, full_name in enumerate(names_series):\n",
    "  formatted_prompt = prompt_template.format(full_name=full_name)\n",
    "  token_est = count_tokens(formatted_prompt)\n",
    "  messages = [HumanMessage(content=formatted_prompt)]\n",
    "  tasks.append((idx, messages, token_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33e27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_batch_task(tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44beced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, resp in results:\n",
    "    print(task_id, parse_json(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2855c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(\"lib/data/name_token_map.jsonl\")\n",
    "with out_path.open(\"w\", encoding='utf-8') as f:\n",
    "    for task_id, resp in results:\n",
    "        try:\n",
    "            parser_json = parse_json(resp)\n",
    "            f.write(json.dumps({\n",
    "                \"full_name\": names_series[task_id],\n",
    "                \"tokens\": parser_json['tokens']\n",
    "            }) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Bad JSON for row {task_id}: {resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f421d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file  = Path(\"lib/data/name_token_map.jsonl\")\n",
    "output_file = Path(\"lib/data/token_map.jsonl\")\n",
    "\n",
    "# Step 1: Build token_map\n",
    "token_map = defaultdict(set)\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        full_name = entry[\"full_name\"].strip()   # remove leading/trailing spaces\n",
    "        tokens = entry[\"tokens\"]\n",
    "\n",
    "        for token in tokens:\n",
    "            # Optional: normalize token if needed\n",
    "            normalized_token = token.strip().lower()\n",
    "            token_map[normalized_token].add(full_name)\n",
    "\n",
    "# Step 2: Write out as JSONL (deduplicated automatically via set)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for token, names in token_map.items():\n",
    "        # Sort for consistency (optional)\n",
    "        unique_names = sorted(names)\n",
    "        f.write(json.dumps({token: unique_names}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b97a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "stacked = build_name_dict(temp_df)\n",
    "names_series = stacked['addr']\n",
    "\n",
    "token_map = defaultdict(set)\n",
    "word_re = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)*\")\n",
    "for full in names_series:\n",
    "    cleaned = re.sub(\n",
    "        r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\n",
    "        \"\",\n",
    "        full\n",
    "    )\n",
    "    for token in word_re.findall(cleaned.lower()):\n",
    "        token_map[token].add(full)\n",
    "\n",
    "out_file = Path(\"lib/data/name_token_map.jsonl\")\n",
    "with out_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for token, fulls in token_map.items():\n",
    "        f.write(json.dumps({\"token\": token, \"full_names\": list(fulls)}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c285361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Set\n",
    "import json\n",
    "\n",
    "token_map: Dict[str, Set[str]] = defaultdict(set)\n",
    "\n",
    "with open(\"lib/data/token_map.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        for token, names in entry.items():\n",
    "            token_map[token].update(names)\n",
    "\n",
    "token_map = dict(token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff0a14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Set\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "WORD_RE = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)*\")\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    \"\"\"Normalize separators for comparison.\"\"\"\n",
    "    return re.sub(r\"[-_]+\", \" \", s.lower()).strip()\n",
    "\n",
    "def expand_query(query: str, token_map: Dict[str, Set[str]], fuzzy_threshold: float = 0.75) -> str:\n",
    "    if not token_map:\n",
    "        return query\n",
    "\n",
    "    query_tokens = WORD_RE.findall(query)\n",
    "    expanded_tokens = []\n",
    "\n",
    "    for q_tok in query_tokens:\n",
    "        q_norm = normalize_token(q_tok)\n",
    "        best_full = None\n",
    "        best_score = 0.0\n",
    "\n",
    "        for token, full_names in token_map.items():\n",
    "            token_norm = normalize_token(token)\n",
    "            for full in full_names:\n",
    "                # Compare normalized query token against token and full name\n",
    "                sim_token = SequenceMatcher(None, q_norm, token_norm).ratio()\n",
    "                sim_full = SequenceMatcher(None, q_norm, normalize_token(full)).ratio()\n",
    "                sim = max(sim_token, sim_full)\n",
    "\n",
    "                if sim > best_score:\n",
    "                    best_score = sim\n",
    "                    best_full = full\n",
    "\n",
    "        if best_full and best_score >= fuzzy_threshold:\n",
    "            expanded_tokens.append(best_full)\n",
    "        else:\n",
    "            expanded_tokens.append(q_tok)\n",
    "\n",
    "    return \" \".join(expanded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "519882c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need details msme chamber of commerce and industry of india msmeglobalccii@msmeccii.in sankar narayanan sankar.narayanan@2getherments.com gvvsl narayana gvvslnarayana@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(expand_query(query=\"need details of sankar narayan\", token_map=token_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc44bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from typing import Dict, Set, Optional, Tuple\n",
    "\n",
    "_HAVE_RAPIDFUZZ = True\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # remove angle/round-bracketed extras and email localparts\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\S+@\\S+\", \" \", s)\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "\n",
    "    sender_norm = _normalize(sender)\n",
    "\n",
    "    # Fast path: exact token key match (case-insensitive)\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: _score(sender_norm, _normalize(f))\n",
    "                )\n",
    "                best_score = _score(sender_norm, _normalize(best_full))\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "\n",
    "    # Build candidate list (normalized strings) -> meta mapping\n",
    "    candidates = []\n",
    "    meta = {}  # candidate_norm -> (token_key, full_name_or_None)\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = _normalize(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = _normalize(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # If rapidfuzz available, use extractOne with a strong scorer (WRatio)\n",
    "    if _HAVE_RAPIDFUZZ:\n",
    "        # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "        unique_choices = list(dict.fromkeys(candidates))\n",
    "        match = process.extractOne(\n",
    "            sender_norm,\n",
    "            unique_choices,\n",
    "            scorer=fuzz.WRatio,\n",
    "            score_cutoff=threshold\n",
    "        )\n",
    "        if not match:\n",
    "            return None\n",
    "        match_str, score, _ = match  # match_str is normalized candidate\n",
    "        token_key, full_name = meta.get(match_str, (None, None))\n",
    "        # If the match candidate was just a token key (full_name is None),\n",
    "        # pick the best full_name under that token_key\n",
    "        if full_name is None and token_key is not None:\n",
    "            best_full = max(\n",
    "                token_map[token_key],\n",
    "                key=lambda f: fuzz.WRatio(sender_norm, _normalize(f))\n",
    "            )\n",
    "            best_score = fuzz.WRatio(sender_norm, _normalize(best_full))\n",
    "            return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "        return (full_name, float(score))\n",
    "\n",
    "    # Fallback: iterate and use SequenceMatcher ratio\n",
    "    best_full = None\n",
    "    best_score = 0.0\n",
    "    for token_key, full_names in token_map.items():\n",
    "        token_norm = _normalize(token_key)\n",
    "        # score against token key\n",
    "        token_score = max(_seq_ratio(sender_norm, token_norm), 0.0)\n",
    "        if token_score * 100 > best_score:\n",
    "            # if token seems promising check its full names\n",
    "            for full in full_names:\n",
    "                full_norm = _normalize(full)\n",
    "                s = _seq_ratio(sender_norm, full_norm) * 100\n",
    "                if s > best_score:\n",
    "                    best_score = s\n",
    "                    best_full = full\n",
    "        # also compare sender directly to each full_name\n",
    "        for full in full_names:\n",
    "            full_norm = _normalize(full)\n",
    "            s = _seq_ratio(sender_norm, full_norm) * 100\n",
    "            if s > best_score:\n",
    "                best_score = s\n",
    "                best_full = full\n",
    "\n",
    "    if best_full and best_score >= threshold:\n",
    "        return best_full, best_score\n",
    "    return None\n",
    "\n",
    "# helper scoring functions (used by fallback and for small composition)\n",
    "def _seq_ratio(a: str, b: str) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()  # returned 0..1\n",
    "\n",
    "def _score(a_norm: str, b_norm: str) -> float:\n",
    "    \"\"\"Return 0..100 score using rapidfuzz if present, else difflib*100.\"\"\"\n",
    "    if _HAVE_RAPIDFUZZ:\n",
    "        return float(fuzz.WRatio(a_norm, b_norm))\n",
    "    return _seq_ratio(a_norm, b_norm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "722babd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # remove angle/round-bracketed extras and email localparts\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\S+@\\S+\", \" \", s)\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def score(a_norm: str, b_norm: str) -> float:\n",
    "    \"\"\"Return 0..100 score using rapidfuzz if present, else difflib*100.\"\"\"\n",
    "    return float(fuzz.WRatio(a_norm, b_norm))\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "    sender_norm = normalize_text(sender)\n",
    "\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: score(sender_norm, normalize_text(f))\n",
    "                )\n",
    "                best_score = score(sender_norm, normalize_text(best_full))\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "    \n",
    "    candidates = []\n",
    "    meta = {}\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = normalize_text(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = normalize_text(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "    unique_choices = list(dict.fromkeys(candidates))\n",
    "    match = process.extractOne(\n",
    "        sender_norm,\n",
    "        unique_choices,\n",
    "        scorer=fuzz.WRatio,\n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    if not match:\n",
    "        return None\n",
    "    match_str, score, _ = match  # match_str is normalized candidate\n",
    "    token_key, full_name = meta.get(match_str, (None, None))\n",
    "    # If the match candidate was just a token key (full_name is None),\n",
    "    # pick the best full_name under that token_key\n",
    "    if full_name is None and token_key is not None:\n",
    "        best_full = max(\n",
    "            token_map[token_key],\n",
    "            key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))\n",
    "        )\n",
    "        best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))\n",
    "        return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "    return (full_name, float(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "from typing import Dict, Set, Optional, Tuple\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize a name/email for robust matching.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # replace separators with spaces, strip non-alphanumerics\n",
    "    s = re.sub(r\"[-_.]+\", \" \", s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def get_best_match_from_token_map(\n",
    "    sender: str,\n",
    "    token_map: Dict[str, Set[str]],\n",
    "    threshold: int = 75\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return (best_full_name, score) if match >= threshold (0-100), else None.\n",
    "    Uses rapidfuzz when present, otherwise difflib fallback.\n",
    "    \"\"\"\n",
    "    if not sender or not token_map:\n",
    "        return None\n",
    "    sender_norm = normalize_text(sender)\n",
    "\n",
    "    sender_lower = sender.lower().strip()\n",
    "    if sender_lower in (k.lower() for k in token_map.keys()):\n",
    "        # pick best full_name for that token by comparing normalized forms\n",
    "        for k in token_map:\n",
    "            if k.lower() == sender_lower:\n",
    "                best_full = max(\n",
    "                    token_map[k],\n",
    "                    key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))  # fixed\n",
    "                )\n",
    "                best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))  # fixed\n",
    "                if best_score >= threshold:\n",
    "                    return best_full, best_score\n",
    "                return None\n",
    "    \n",
    "    candidates = []\n",
    "    meta = {}\n",
    "    for token_key, full_names in token_map.items():\n",
    "        tnorm = normalize_text(token_key)\n",
    "        if tnorm:\n",
    "            candidates.append(tnorm)\n",
    "            # token candidate references no specific full name (None)\n",
    "            meta[tnorm] = (token_key, None)\n",
    "        for full in full_names:\n",
    "            fnorm = normalize_text(full)\n",
    "            if fnorm:\n",
    "                candidates.append(fnorm)\n",
    "                meta[fnorm] = (token_key, full)\n",
    "\n",
    "    # remove duplicates while preserving meta mapping (last wins but that's okay)\n",
    "    unique_choices = list(dict.fromkeys(candidates))\n",
    "    match = process.extractOne(\n",
    "        sender_norm,\n",
    "        unique_choices,\n",
    "        scorer=fuzz.WRatio,\n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    if not match:\n",
    "        return None\n",
    "    match_str, score, _ = match  # match_str is normalized candidate\n",
    "    token_key, full_name = meta.get(match_str, (None, None))\n",
    "    # If the match candidate was just a token key (full_name is None),\n",
    "    # pick the best full_name under that token_key\n",
    "    if full_name is None and token_key is not None:\n",
    "        best_full = max(\n",
    "            token_map[token_key],\n",
    "            key=lambda f: fuzz.WRatio(sender_norm, normalize_text(f))\n",
    "        )\n",
    "        best_score = fuzz.WRatio(sender_norm, normalize_text(best_full))\n",
    "        return (best_full, float(best_score)) if best_score >= threshold else None\n",
    "    return (full_name, float(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df447647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('venkata kishore kalagotla via team 2getherments team.2getherments@2getherments.com', 90.0)\n"
     ]
    }
   ],
   "source": [
    "sender = 'kishore kalagotla'\n",
    "if sender:\n",
    "    sender = sender.lower()\n",
    "    res = get_best_match_from_token_map(sender, token_map, threshold=75)\n",
    "    if res:\n",
    "        print(res)\n",
    "        best_full_name, score = res\n",
    "        sender = best_full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee17f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "989d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = \"kishore kalagotla 2getherments\"\n",
    "\n",
    "mask = pl.lit(True)\n",
    "temp_df = temp_df.with_columns([\n",
    "    pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "])\n",
    "\n",
    "sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "mask = mask & sender_mask\n",
    "\n",
    "temp_df = temp_df.filter(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "812e75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kishore kalagotla  kishore.kalagotla@2getherments.com\n"
     ]
    }
   ],
   "source": [
    "print(temp_df['from_normalized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4bfdaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "value = \"sankar.narayanan@2getherments.com\"\n",
    "column = \"sankar narayanan sankar.narayanan@2getherments.com\"\n",
    "\n",
    "print(fuzz.partial_ratio(value.lower(), column.lower()) > 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b850dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import normalize_list, match_value_in_columns, get_best_match_from_token_map\n",
    "from lib.load_data import df, token_map\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04663d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_subject(subject: str) -> str:\n",
    "    if not isinstance(subject, str):\n",
    "        return \"\"\n",
    "    subject = re.sub(r'^(re|fwd|fw):\\s*', '', subject, flags=re.I)  # remove reply/forward\n",
    "    return subject.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f4ec47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refund statement - 805\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_subject(\"Re: Refund Statement - 805\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07f7f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(text: str) -> set[str]:\n",
    "    return set(re.findall(r'\\b\\d+\\b', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fb44a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'805'}\n"
     ]
    }
   ],
   "source": [
    "print(extract_numbers(\"refund statement - 805\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3cedc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def preprocess_subject(subject: str) -> str:\n",
    "    if not isinstance(subject, str):\n",
    "        return \"\"\n",
    "    # Lowercase and replace symbols with space\n",
    "    subject = re.sub(r'[:\\-_,]', ' ', subject)\n",
    "    subject = re.sub(r'\\s+', ' ', subject)  # normalize spaces\n",
    "    return subject.lower().strip()\n",
    "\n",
    "def extract_numbers(text: str) -> set[str]:\n",
    "    return set(re.findall(r'\\b\\d+\\b', text))\n",
    "\n",
    "def smart_subject_match(user_value: str, column_value: str) -> bool:\n",
    "    if not column_value:\n",
    "        return False\n",
    "    \n",
    "    user_clean = preprocess_subject(user_value)\n",
    "    col_clean = preprocess_subject(column_value)\n",
    "\n",
    "    user_nums = extract_numbers(user_clean)\n",
    "    col_nums = extract_numbers(col_clean)\n",
    "\n",
    "    # --- Number must match if present ---\n",
    "    if user_nums and not (user_nums & col_nums):\n",
    "        return False\n",
    "\n",
    "    # --- Fuzzy match on remaining text ---\n",
    "    fuzz_score = fuzz.token_set_ratio(user_clean, col_clean) / 100\n",
    "\n",
    "    if user_nums:\n",
    "        # numbers match â†’ relax threshold\n",
    "        return fuzz_score >= 0.65\n",
    "    else:\n",
    "        # no numbers â†’ require stricter match\n",
    "        return fuzz_score >= 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "efbae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pl.lit(True)\n",
    "subject = \"Refund Statement 805\"\n",
    "# \"Refund for Flat no: 805\"\n",
    "# \"Re: Refund Statement - 805\"\n",
    "temp_df = df.clone()\n",
    "\n",
    "if subject:    \n",
    "    subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "    mask = mask & subject_mask\n",
    "\n",
    "temp_df = temp_df.filter(mask)\n",
    "\n",
    "total_matches = temp_df.height\n",
    "preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "\n",
    "results_preview = temp_df.head(5).select(preview_cols).to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb922765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '198f083a5ce11830', 'threadId': '198f02e745824fb8', 'from': 'Sankar Narayanan <sankar.narayanan@2getherments.com>', 'to': ['nirav.j05@gmail.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T17:20:03+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': '@Rama Krishna Please proceed further. On Thu, Aug 28, 2025 at 5:05 PM Nirav Joshi &lt;nirav.j05@gmail.com&gt; wrote: Thank you Sankar sir for your swift response. Please proceed with the same. On Thu,', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '199098a3358031c1', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-09-02T13:57:42+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com', 'arshkaur19@gmail.com'], 'snippet': 'Hi Rama sir, Please update status on this request. Thank you. On Thu, Aug 28, 2025 at 5:20 PM Sankar Narayanan &lt;sankar.narayanan@2getherments.com&gt; wrote: @Rama Krishna Please proceed further. On', 'labels': ['IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f0418716eeeef', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T16:07:48+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Dear Sankar sir, Can you please share the Excel file and there is Total missing in above email can you please make corrections and share with us. On Thu, Aug 28, 2025 at 3:47 PM Sankar Narayanan &lt;', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f02e745824fb8', 'threadId': '198f02e745824fb8', 'from': 'Sankar Narayanan <sankar.narayanan@2getherments.com>', 'to': ['nirav.j05@gmail.com'], 'subject': 'Refund Statement - 805', 'date': '2025-08-28T15:47:00+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Dear Nirav, As discussed,Please find the refund statement attached for your reference. Kindly Acknowledge to proceed further. Shankar.S Manager-Customer Relations Ph:63669 55995 805 Refund Statement S.', 'labels': ['UNREAD', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n",
      "{'id': '198f075c6bc02c3c', 'threadId': '198f02e745824fb8', 'from': 'Nirav Joshi <nirav.j05@gmail.com>', 'to': ['sankar.narayanan@2getherments.com'], 'subject': 'Re: Refund Statement - 805', 'date': '2025-08-28T17:04:52+05:30Z', 'cc': ['hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'ramakrishna@2getherments.com', 'shachiraju@2getherments.com'], 'snippet': 'Thank you Sankar sir for your swift response. Please proceed with the same. On Thu, Aug 28, 2025 at 4:07 PM Nirav Joshi &lt;nirav.j05@gmail.com&gt; wrote: Dear Sankar sir, Can you please share the', 'labels': ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX'], 'attachments': []}\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for result in results_preview:\n",
    "    print(result)\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb97eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import normalize_list, match_value_in_columns, smart_subject_match\n",
    "from lib.load_data import df\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl\n",
    "from typing import Union\n",
    "\n",
    "# @tool(\"email_filtering_tool\", parse_docstring=True)\n",
    "def email_filtering_tool(\n",
    "    uid: str = None,\n",
    "    threadId: str = None,\n",
    "    sender: str = None,\n",
    "    recipient: str = None,\n",
    "    subject: str = None,\n",
    "    cc: bool = False,\n",
    "    labels: list[str] = None,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "    body: bool = False,\n",
    "    html: bool = False,\n",
    "    sort_by: str = \"date\",\n",
    "    sort_order: str = \"desc\",\n",
    "    limit: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This tool filter emails based on metadata such as sender (human), recipient (human), date range, or thread ID.\n",
    "    \n",
    "    Args:\n",
    "        uid (str, optional): Filter emails by their unique UID. Exact match required.\n",
    "        threadId: Filter emails by their conversation (email chian) thread ID, Returns all messages belonging to that specific chain (thread).\n",
    "        sender (str or list of str, optional): Filter emails by sender(s). Can be full email address, partial email, or sender names (case-insensitive, only humans).\n",
    "        recipient (str or list of str, optional): Filter emails by recipient(s). Can be full email addresses, partial emails, or recipient names, but strictly not numbers. (case-insensitive, only humans).\n",
    "        subject (str, optional): Filter email by subject text. Can be full or partial subject string (case-insensitive).\n",
    "        cc (bool, optional): Filter cc recepients of the email only when explicitly requested. Default False.\n",
    "        labels (list of str, optional): Filter emails by one or more labels. Matches any email that contains at least one of the provided labels (case-insensitive).\n",
    "        start_date (str, optional): Filter emails sent on or after this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        end_date (str, optional): Filter emails sent on or before this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        body (bool, optional): Include the plain-text email body only when explicitly requested. Default False.\n",
    "        html (bool, optional): Include the full HTML body only when explicitly requested. Default False.\n",
    "        sort_by (str, optional): Column to sort the results by. Default is 'date'.\n",
    "        sort_order (str, optional): Sort order: 'asc' for ascending, 'desc' for descending. Default is 'desc'.\n",
    "        limit (int, optional): Maximum number of results to return. Default is 5.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"email_filtering_tool is being called {uid}, {threadId}, {sender}, {recipient}, {subject}, {cc}, {labels}, {start_date}, {end_date}, {body}, {html}, {sort_by}, {sort_order}, {limit}\")\n",
    "    temp_df = df.clone()\n",
    "    mask = pl.lit(True)\n",
    "\n",
    "    if uid:\n",
    "        mask = mask & (pl.col(\"id\") == uid)\n",
    "\n",
    "    if threadId:\n",
    "        mask = mask & (pl.col(\"threadId\") == threadId)\n",
    "\n",
    "    # --- Sender filter (case-insensitive, matches name or email) ---\n",
    "    if sender:\n",
    "        sender = sender.lower()\n",
    "        # Add a normalized column\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "        ])\n",
    "        # Filter rows where the normalized 'from' matches sender\n",
    "        sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "        mask = mask & sender_mask\n",
    "\n",
    "    # --- Recipient filter ---\n",
    "    if recipient:\n",
    "        recipient = recipient.lower()\n",
    "        # Normalize 'to' and 'cc' columns which are lists\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "        ])\n",
    "        # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "        recipient_mask = (\n",
    "            pl.col(\"to_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "        )\n",
    "        if cc:\n",
    "            # Normalize 'to' and 'cc' columns which are lists\n",
    "            temp_df = temp_df.with_columns([\n",
    "                pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "            ])\n",
    "            # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "            cc_mask = (\n",
    "                pl.col(\"cc_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "            )\n",
    "            recipient_mask = recipient_mask | cc_mask\n",
    "\n",
    "        mask = mask & recipient_mask\n",
    "\n",
    "    # --- Date filtering (normalize to datetime) ---\n",
    "    if start_date or end_date:\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%SZ\", strict=False).alias(\"date_dt\")\n",
    "        ])\n",
    "        \n",
    "    if start_date:\n",
    "        try:\n",
    "            start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "            mask = mask & (pl.col(\"date_dt\") >= start_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing start_date: {e}\"\n",
    "\n",
    "    if end_date:\n",
    "        try:\n",
    "            end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1) - timedelta(seconds=1)\n",
    "            mask = mask & (pl.col(\"date_dt\") <= end_date_dt)\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing end_date: {e}\"\n",
    "        \n",
    "    if labels: \n",
    "        labels = [lbl.strip().lower() for lbl in labels]\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"lables\").map_elements(normalize_list, return_dtype=str).alias(\"labels_normalized\")\n",
    "        ])\n",
    "\n",
    "        labels_mask = pl.col(\"labels_normalized\").map_elements(\n",
    "            lambda email_lables: any(lbl in email_lables for lbl in labels),\n",
    "            return_dtype=bool\n",
    "        )\n",
    "\n",
    "        mask = mask & labels_mask\n",
    "\n",
    "    if subject:    \n",
    "        subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "        mask = mask & subject_mask\n",
    "\n",
    "    # Apply the mask only once\n",
    "    temp_df = temp_df.filter(mask)\n",
    "\n",
    "    # --- Sorting ---\n",
    "    temp_df = temp_df.sort(\n",
    "        by=sort_by,\n",
    "        descending=(sort_order.lower() == \"desc\")\n",
    "    )\n",
    "\n",
    "    # --- Handle empty result ---\n",
    "    if temp_df.is_empty():\n",
    "        return \"No emails found matching the specified criteria.\"\n",
    "\n",
    "    # --- Preview results ---\n",
    "    total_matches = temp_df.height\n",
    "    preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "    if body:\n",
    "        preview_cols.append(\"body\")\n",
    "    if html:\n",
    "        preview_cols.append(\"html\")\n",
    "\n",
    "    if limit is None:\n",
    "        results_preview = temp_df.select(preview_cols).to_dicts()\n",
    "    else:\n",
    "        results_preview = temp_df.head(limit).select(preview_cols).to_dicts()\n",
    "\n",
    "    def fmt(res):\n",
    "        parts = [\n",
    "            f\"id: {res.get('id','N/A')}\",\n",
    "            f\"ThreadId: {res.get('threadId','N/A')}\",\n",
    "            f\"From: {res.get('from','N/A')}\",\n",
    "            f\"To: {res.get('to','N/A')}\",\n",
    "            f\"CC: {res.get('cc','N/A')}\",\n",
    "            f\"Subject: {res.get('subject','N/A')}\",\n",
    "            f\"Date: {res.get('date','N/A')}\",\n",
    "            f\"Snippet: {res.get('snippet','N/A')}\",\n",
    "            f\"Labels: {res.get('labels','N/A')}\",\n",
    "            f\"Attachments: {res.get('attachments','N/A')}\",\n",
    "        ]\n",
    "        if body:\n",
    "            parts.append(f\"Body:\\n{res.get('body','N/A')}\")\n",
    "        if html:\n",
    "            parts.append(f\"HTML:\\n{res.get('html','N/A')}\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    formatted_results = \"\\n\\n---\\n\\n\".join(fmt(r) for r in results_preview)\n",
    "    shown = total_matches if limit is None else min(int(limit), total_matches)\n",
    "    return f\"Found {total_matches} emails matching the criteria. Showing {shown}:\\n\\n{formatted_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d47d2038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Re: Refund Statement - 805'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efed0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "from lib.load_data import df\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from lib.utils import normalize_list, match_value_in_columns, smart_subject_match\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert email summarizer.  \n",
    "\n",
    "Task:  \n",
    "- Input: Multiple emails with metadata (id, threadId, from, to, cc, Subject, date, snippet, body, labels, attachments).  \n",
    "- Group by ThreadId and summarize chronologically.  \n",
    "- Capture key points, actions, and important details with clarity and brevity.\n",
    "\n",
    "Summarize this,\n",
    "{chunk}\n",
    "\"\"\"\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_completion_tokens=512\n",
    ")\n",
    "\n",
    "encoding_model = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def get_chunks(text: str, chunk_size: int = 10000) -> List[str]:\n",
    "    \"\"\"Split a large text into token-based chunks.\"\"\"\n",
    "    tokens = encoding_model.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunk_text = encoding_model.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding_model.enc(text))\n",
    "\n",
    "def run_batch_task(tasks: List[Tuple[int, List[HumanMessage], int]], tpm_limit: int = 29000) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    tasks: list of (task_id, messages, est_tokens)\n",
    "    tpm_limit: max tokens/minute allowed\n",
    "    returns: list of (task_id, response_text)\n",
    "    \"\"\"\n",
    "    results: List[Tuple[int, str]] = []\n",
    "    current_batch: List[Tuple[int, List[HumanMessage], int]] = []\n",
    "    current_tokens = 0\n",
    "    window_start = time.time()\n",
    "\n",
    "    def flush(batch):\n",
    "        \"\"\"Send a batch to the LLM and record results.\"\"\"\n",
    "        nonlocal results\n",
    "        if not batch:\n",
    "            return\n",
    "        responses = llm.batch([msgs for _, msgs, _ in batch])\n",
    "        for (task_id, _, _), resp in zip(batch, responses):\n",
    "            results.append((task_id, resp.content))\n",
    "\n",
    "    for task in tasks:\n",
    "        _, _, tok = task\n",
    "\n",
    "        if current_tokens + tok > tpm_limit and current_batch:\n",
    "            flush(current_batch)\n",
    "            current_batch, current_tokens = [], 0\n",
    "\n",
    "            # respect TPM limit\n",
    "            elapsed = time.time() - window_start\n",
    "            if elapsed < 60:\n",
    "                time.sleep(60 - elapsed)\n",
    "            window_start = time.time()\n",
    "\n",
    "        current_batch.append(task)\n",
    "        current_tokens += tok\n",
    "\n",
    "    if current_batch:\n",
    "        flush(current_batch)\n",
    "\n",
    "    return results\n",
    "\n",
    "def parse_datetime_utc(date_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Parse input date string and return a UTC-aware datetime object.\n",
    "    Accepts 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    if len(date_str) == 10:  # date-only\n",
    "        dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    else:  # full datetime\n",
    "        dt = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    # Make it UTC-aware\n",
    "    return dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def human_readable_date(timestamp) -> str:\n",
    "    \"\"\"\n",
    "    Convert a timestamp to human-readable form.\n",
    "    Accepts: str, datetime.datetime, or None\n",
    "    \"\"\"\n",
    "    if timestamp is None:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    # If Polars datetime, convert to Python datetime\n",
    "    if not isinstance(timestamp, datetime):\n",
    "        try:\n",
    "            # Try parsing string\n",
    "            timestamp = datetime.fromisoformat(str(timestamp))\n",
    "        except Exception:\n",
    "            return \"N/A\"\n",
    "    \n",
    "    return timestamp.strftime(\"%a, %b %d, %Y %I:%M %p\")\n",
    "\n",
    "# @tool(\"email_filtering_tool\", parse_docstring=True)\n",
    "def email_filtering_tool(\n",
    "    uid: str = None,\n",
    "    threadId: str = None,\n",
    "    sender: str = None,\n",
    "    recipient: str = None,\n",
    "    subject: str = None,\n",
    "    cc: bool = False,\n",
    "    labels: list[str] = None,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "    body: bool = False,\n",
    "    html: bool = False,\n",
    "    sort_by: str = \"date\",\n",
    "    sort_order: str = \"desc\",\n",
    "    limit: int = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This tool filter emails based on metadata such as sender (human), recipient (human), date range, or thread ID.\n",
    "    \n",
    "    Args:\n",
    "        uid (str, optional): Filter emails by their unique UID. Exact match required.\n",
    "        threadId: Filter emails by their conversation (email chian) thread ID, Returns all messages belonging to that specific chain (thread).\n",
    "        sender (str or list of str, optional): Filter emails by sender(s). Can be full email address, partial email, or sender names (case-insensitive, only humans).\n",
    "        recipient (str or list of str, optional): Filter emails by recipient(s). Can be full email addresses, partial emails, or recipient names, but strictly not numbers. (case-insensitive, only humans).\n",
    "        subject (str, optional): Filter email by subject text. Can be full or partial subject string (case-insensitive).\n",
    "        cc (bool, optional): Filter cc recepients of the email only when explicitly requested. Default False.\n",
    "        labels (list of str, optional): Filter emails by one or more labels. Matches any email that contains at least one of the provided labels (case-insensitive).\n",
    "        start_date (str, optional): Filter emails sent on or after this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        end_date (str, optional): Filter emails sent on or before this date. Format: 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS'.\n",
    "        body (bool, optional): Include the plain-text email body only when explicitly requested. Default False.\n",
    "        html (bool, optional): Include the full HTML body only when explicitly requested. Default False.\n",
    "        sort_by (str, optional): Column to sort the results by. Default is 'date_dt'.\n",
    "        sort_order (str, optional): Sort order: 'asc' for ascending, 'desc' for descending. Default is 'desc'.\n",
    "        limit (int, optional): Maximum number of results to return. set default value to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"email_filtering_tool is being called {uid}, {threadId}, {sender}, {recipient}, {subject}, {cc}, {labels}, {start_date}, {end_date}, {body}, {html}, {sort_by}, {sort_order}, {limit}\")\n",
    "    temp_df = df.clone()\n",
    "    mask = pl.lit(True)\n",
    "\n",
    "    temp_df = temp_df.with_columns([\n",
    "        temp_df[\"body\"].struct.field(\"text\").alias(\"body_text\"),\n",
    "        temp_df[\"body\"].struct.field(\"html\").alias(\"body_html\"),\n",
    "    ])\n",
    "\n",
    "    if uid:\n",
    "        mask = mask & (pl.col(\"id\") == uid)\n",
    "\n",
    "    if threadId:\n",
    "        mask = mask & (pl.col(\"threadId\") == threadId)\n",
    "\n",
    "    # --- Sender filter (case-insensitive, matches name or email) ---\n",
    "    if sender:\n",
    "        sender = sender.lower()\n",
    "        # Add a normalized column\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"from\").map_elements(normalize_list, return_dtype=str).alias(\"from_normalized\")\n",
    "        ])\n",
    "        # Filter rows where the normalized 'from' matches sender\n",
    "        sender_mask = pl.col(\"from_normalized\").map_elements(lambda x: match_value_in_columns(sender, x), return_dtype=bool)\n",
    "        mask = mask & sender_mask\n",
    "\n",
    "    # --- Recipient filter ---\n",
    "    if recipient:\n",
    "        recipient = recipient.lower()\n",
    "        # Normalize 'to' and 'cc' columns which are lists\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"to\").map_elements(normalize_list, return_dtype=str).alias(\"to_normalized\")\n",
    "        ])\n",
    "        # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "        recipient_mask = (\n",
    "            pl.col(\"to_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "        )\n",
    "        if cc:\n",
    "            # Normalize 'to' and 'cc' columns which are lists\n",
    "            temp_df = temp_df.with_columns([\n",
    "                pl.col(\"cc\").map_elements(normalize_list, return_dtype=str).alias(\"cc_normalized\")\n",
    "            ])\n",
    "            # Filter rows where any normalized 'to' or 'cc' matches the recipient\n",
    "            cc_mask = (\n",
    "                pl.col(\"cc_normalized\").map_elements(lambda x: match_value_in_columns(recipient, x), return_dtype=bool)\n",
    "            )\n",
    "            recipient_mask = recipient_mask | cc_mask\n",
    "\n",
    "        mask = mask & recipient_mask\n",
    "\n",
    "    # --- Date filtering (normalize to datetime) ---\n",
    "    temp_df = temp_df.with_columns(\n",
    "        pl.col(\"date\")\n",
    "        .str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\", strict=False)\n",
    "        .dt.convert_time_zone(\"UTC\")\n",
    "        .alias(\"date_dt\")\n",
    "    )\n",
    "\n",
    "    if start_date:\n",
    "        start_dt = parse_datetime_utc(start_date)\n",
    "        mask = mask & (pl.col(\"date_dt\") >= start_dt)\n",
    "\n",
    "    if end_date:\n",
    "        end_dt = parse_datetime_utc(end_date)\n",
    "        # If only date provided, include the full day\n",
    "        if len(end_date) == 10:\n",
    "            end_dt = end_dt + timedelta(days=1) - timedelta(seconds=1)\n",
    "        mask = mask & (pl.col(\"date_dt\") <= end_dt)\n",
    "\n",
    "    if labels: \n",
    "        labels = [lbl.strip().lower() for lbl in labels]\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(\"lables\").map_elements(normalize_list, return_dtype=str).alias(\"labels_normalized\")\n",
    "        ])\n",
    "\n",
    "        labels_mask = pl.col(\"labels_normalized\").map_elements(\n",
    "            lambda email_lables: any(lbl in email_lables for lbl in labels),\n",
    "            return_dtype=bool\n",
    "        )\n",
    "\n",
    "        mask = mask & labels_mask\n",
    "\n",
    "    if subject:    \n",
    "        subject_mask = pl.col(\"subject\").map_elements(lambda x: smart_subject_match(subject, x), return_dtype=bool)\n",
    "        mask = mask & subject_mask\n",
    "\n",
    "    # Apply the mask only once\n",
    "    temp_df = temp_df.filter(mask)\n",
    "\n",
    "    # --- Sorting ---\n",
    "    temp_df = temp_df.sort(\n",
    "        by=sort_by,\n",
    "        descending=(sort_order.lower() == \"desc\")\n",
    "    )\n",
    "\n",
    "    # --- Handle empty result ---\n",
    "    if temp_df.is_empty():\n",
    "        return \"No emails found matching the specified criteria.\"\n",
    "\n",
    "    # --- Preview results ---\n",
    "    total_matches = temp_df.height\n",
    "    preview_cols = [\"id\", \"threadId\", \"from\", \"to\", \"subject\", \"date_dt\", \"cc\", \"snippet\", \"labels\", \"attachments\"]\n",
    "    if body:\n",
    "        preview_cols.append(\"body_text\")\n",
    "    if html:\n",
    "        preview_cols.append(\"body_html\")\n",
    "\n",
    "    if limit is None:\n",
    "        results_preview = temp_df.select(preview_cols).to_dicts()\n",
    "    else:\n",
    "        results_preview = temp_df.head(limit).select(preview_cols).to_dicts()\n",
    "\n",
    "    def fmt(res):\n",
    "        parts = [\n",
    "            f\"id: {res.get('id','N/A')}\",\n",
    "            f\"ThreadId: {res.get('threadId','N/A')}\",\n",
    "            f\"From: {res.get('from','N/A')}\",\n",
    "            f\"To: {res.get('to','N/A')}\",\n",
    "            f\"CC: {res.get('cc','N/A')}\",\n",
    "            f\"Subject: {res.get('subject','N/A')}\",\n",
    "            f\"Date: {human_readable_date(res.get('date_dt'))}\",\n",
    "            f\"Snippet: {res.get('snippet','N/A')}\",\n",
    "            f\"Labels: {res.get('labels','N/A')}\",\n",
    "            f\"Attachments: {res.get('attachments','N/A')}\",\n",
    "        ]\n",
    "        if body:\n",
    "            parts.append(f\"Body: {res.get('body_text','N/A')}\")\n",
    "        if html:\n",
    "            parts.append(f\"HTML: {res.get('body_html','N/A')}\")\n",
    "        return \"\\n\".join(parts)\n",
    "    \n",
    "    formatted_results = \"\\n\\n---\\n\\n\".join(fmt(r) for r in results_preview)\n",
    "    shown = total_matches if limit is None else min(int(limit), total_matches)\n",
    "    return f\"Found {total_matches} emails matching the criteria. Showing {shown}:\\n\\n{formatted_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c0d4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_filtering_tool is being called 198ca6688975b1b3, None, None, None, None, False, None, None, None, True, False, date_dt, desc, 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Found 1 emails matching the criteria. Showing 1:\\n\\nid: 198ca6688975b1b3\\nThreadId: 197eed51c0afe726\\nFrom: Santosh Turamari <santoshbt@outlook.com>\\nTo: ['sankar.narayanan@2getherments.com']\\nCC: ['ramakrishna@2getherments.com', 'hari@2getherments.com', 'pallavi@2getherments.com', 'meena@2getherments.com', 'customer.communications@2getherments.com', 'shachiraju@2getherments.com']\\nSubject: Re: Refund Statement - 711\\nDate: N/A\\nSnippet: Hi Ramakrishna, I have given the account details to transfer the refund amount. You can deduct the interior deposit from the same. Please refund 22412 Rs this week. Confirm once transferred. Thanks,\\nLabels: ['UNREAD', 'IMPORTANT', 'CATEGORY_PERSONAL', 'INBOX']\\nAttachments: []\\nBody: Hi Ramakrishna,\\r\\n\\r\\nI have given the account details to transfer the refund amount.\\r\\nYou can deduct the interior deposit from the same.\\r\\nPlease refund 22,412 Rs this week.\\r\\n\\r\\nConfirm once transferred.\\r\\n\\r\\nThanks,\\r\\nSantosh T\\r\\n________________________________\\r\\nFrom: Santosh Turamari <santoshbt@outlook.com>\\r\\nSent: 03 August 2025 22:31\\r\\nTo: Sankar Narayanan <sankar.narayanan@2getherments.com>\\r\\nCc: Rama Krishna <ramakrishna@2getherments.com>; Harinath Rao <hari@2getherments.com>; Pallavi T <pallavi@2getherments.com>; Meena T <meena@2getherments.com>; Customer Communications 2g CX <customer.communications@2getherments.com>; Shachiraju Sharma K <shachiraju@2getherments.com>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\nHi,\\r\\n\\r\\nNot yet received the refund.\\r\\nAlso please do not deduct any TDS on this refund as I have not taken any service for these.\\r\\n\\r\\nThanks\\r\\n________________________________\\r\\nFrom: Sankar Narayanan <sankar.narayanan@2getherments.com>\\r\\nSent: 23 July 2025 15:45\\r\\nTo: Santosh Turamari <santoshbt@outlook.com>\\r\\nCc: Rama Krishna <ramakrishna@2getherments.com>; Harinath Rao <hari@2getherments.com>; Pallavi T <pallavi@2getherments.com>; Meena T <meena@2getherments.com>; Customer Communications 2g CX <customer.communications@2getherments.com>; Shachiraju Sharma K <shachiraju@2getherments.com>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\n@Rama Krishna<mailto:ramakrishna@2getherments.com>  Please proceed further.\\r\\n\\r\\nOn Wed, Jul 23, 2025 at 3:44\\u202fPM Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>> wrote:\\r\\nSure thanks\\r\\n\\r\\nGet Outlook for iOS<https://aka.ms/o0ukef>\\r\\n________________________________\\r\\nFrom: Sankar Narayanan <sankar.narayanan@2getherments.com<mailto:sankar.narayanan@2getherments.com>>\\r\\nSent: Wednesday, July 23, 2025 11:10:51 AM\\r\\nTo: Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>>\\r\\nCc: Rama Krishna <ramakrishna@2getherments.com<mailto:ramakrishna@2getherments.com>>; Harinath Rao <hari@2getherments.com<mailto:hari@2getherments.com>>; Pallavi T <pallavi@2getherments.com<mailto:pallavi@2getherments.com>>; Meena T <meena@2getherments.com<mailto:meena@2getherments.com>>; Customer Communications 2g CX <customer.communications@2getherments.com<mailto:customer.communications@2getherments.com>>; Shachiraju Sharma K <shachiraju@2getherments.com<mailto:shachiraju@2getherments.com>>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\n\\r\\nDear Santosh,\\r\\n\\r\\nPlease find attached the updated refund statement for your reference.\\r\\n\\r\\nKindly note that the grill amount has been revised, and we have ensured that the wash basin amounts are accurate as per the standard 2G rates.\\r\\n\\r\\nIf you have any questions or require further clarification, please feel free to connect over a call.\\r\\n\\r\\n\\r\\n@Rama Krishna<mailto:ramakrishna@2getherments.com> Please proceed further.\\r\\n\\r\\n\\r\\n711 FLAT REFUND STATEMENT\\r\\n<http://s.no/>S.NO<http://s.no/>        DESCRIPTION OF WORK     UNITS/SFT       Nos     RATE    AMOUNT\\r\\n1       Wash Basin including Installations              1       4332    4332\\r\\n2       Kitchen Granite (Material & Installations)                              5080\\r\\n3       Grills                          23000\\r\\nGrand Total                             32412\\r\\n\\r\\n\\r\\nShankar.S\\r\\nManager-Customer Relations\\r\\nPh:63669 55995\\r\\n\\r\\n\\r\\n\\r\\nOn Wed, Jul 23, 2025 at 10:54\\u202fAM Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>> wrote:\\r\\nPlease share the updated statement before refund.\\r\\n\\r\\nAc/Type :- saving\\r\\n\\r\\nAc/Number :- 026901515643\\r\\n\\r\\nBranch Address :- ICICI BANK LTD., NO. 102 K H ROAD --- KARNATAKA BANGALORE 560027\\r\\n\\r\\nIFSC Code :- ICIC0001937\\r\\n\\r\\nAlternatively you can do upi at 9902825673\\r\\n\\r\\nThanks\\r\\nSantosh T\\r\\n711\\r\\n________________________________\\r\\nFrom: Rama Krishna <ramakrishna@2getherments.com<mailto:ramakrishna@2getherments.com>>\\r\\nSent: 23 July 2025 10:27\\r\\nTo: Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>>\\r\\nCc: Sankar Narayanan <sankar.narayanan@2getherments.com<mailto:sankar.narayanan@2getherments.com>>; Harinath Rao <hari@2getherments.com<mailto:hari@2getherments.com>>; Pallavi T <pallavi@2getherments.com<mailto:pallavi@2getherments.com>>; Meena T <meena@2getherments.com<mailto:meena@2getherments.com>>; Customer Communications 2g CX <customer.communications@2getherments.com<mailto:customer.communications@2getherments.com>>; Shachiraju Sharma K <shachiraju@2getherments.com<mailto:shachiraju@2getherments.com>>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\nDear Sir,\\r\\n\\r\\nWe will Process the payment\\r\\n\\r\\nPlease share the Bank details and PAN Number.\\r\\n\\r\\nOn Tue, Jul 22, 2025 at 9:28\\u202fPM Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>> wrote:\\r\\nHi,\\r\\n\\r\\nAny update on this?\\r\\n\\r\\nIf I wrote any email to your team, never get any reply?\\r\\n\\r\\nSantosh T\\r\\n711\\r\\n\\r\\nGet Outlook for iOS<https://aka.ms/o0ukef>\\r\\n________________________________\\r\\nFrom: Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>>\\r\\nSent: Thursday, July 10, 2025 5:18:32 PM\\r\\nTo: Sankar Narayanan <sankar.narayanan@2getherments.com<mailto:sankar.narayanan@2getherments.com>>\\r\\nCc: Harinath Rao <hari@2getherments.com<mailto:hari@2getherments.com>>; Pallavi T <pallavi@2getherments.com<mailto:pallavi@2getherments.com>>; Meena T <meena@2getherments.com<mailto:meena@2getherments.com>>; Customer Communications 2g CX <customer.communications@2getherments.com<mailto:customer.communications@2getherments.com>>; Rama Krishna <ramakrishna@2getherments.com<mailto:ramakrishna@2getherments.com>>; Shachiraju Sharma K <shachiraju@2getherments.com<mailto:shachiraju@2getherments.com>>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\nAlso, please process the full refund without deducting in interior work.\\r\\n\\r\\nThanks\\r\\n\\r\\nGet Outlook for iOS<https://aka.ms/o0ukef>\\r\\n________________________________\\r\\nFrom: Santosh Turamari <santoshbt@outlook.com<mailto:santoshbt@outlook.com>>\\r\\nSent: Wednesday, July 9, 2025 6:17:53 PM\\r\\nTo: Sankar Narayanan <sankar.narayanan@2getherments.com<mailto:sankar.narayanan@2getherments.com>>\\r\\nCc: Harinath Rao <hari@2getherments.com<mailto:hari@2getherments.com>>; Pallavi T <pallavi@2getherments.com<mailto:pallavi@2getherments.com>>; Meena T <meena@2getherments.com<mailto:meena@2getherments.com>>; Customer Communications 2g CX <customer.communications@2getherments.com<mailto:customer.communications@2getherments.com>>; Rama Krishna <ramakrishna@2getherments.com<mailto:ramakrishna@2getherments.com>>; Shachiraju Sharma K <shachiraju@2getherments.com<mailto:shachiraju@2getherments.com>>\\r\\nSubject: Re: Refund Statement - 711\\r\\n\\r\\nHi Sankar,\\r\\n\\r\\nGrill is 23,000 also I think was basin with installation looks less. Please review it.\\r\\nonce this is updated, please process the refund.\\r\\n\\r\\nThanks\\r\\n\\r\\nGet Outlook for iOS<https://aka.ms/o0ukef>\\r\\n________________________________\\r\\nFrom: Sankar Narayanan <sankar.narayanan@2getherments.com<mailto:sankar.narayanan@2getherments.com>>\\r\\nSent: Wednesday, July 9, 2025 4:27:02 PM\\r\\nTo: santoshbt@outlook.com<mailto:santoshbt@outlook.com> <santoshbt@outlook.com<mailto:santoshbt@outlook.com>>\\r\\nCc: Harinath Rao <hari@2getherments.com<mailto:hari@2getherments.com>>; Pallavi T <pallavi@2getherments.com<mailto:pallavi@2getherments.com>>; Meena T <meena@2getherments.com<mailto:meena@2getherments.com>>; Customer Communications 2g CX <customer.communications@2getherments.com<mailto:customer.communications@2getherments.com>>; Rama Krishna <ramakrishna@2getherments.com<mailto:ramakrishna@2getherments.com>>; Shachiraju Sharma K <shachiraju@2getherments.com<mailto:shachiraju@2getherments.com>>\\r\\nSubject: Refund Statement - 711\\r\\n\\r\\nDear Santosh,\\r\\nAs discussed,Please find the refund statement attached for your reference.\\r\\nKindly Acknowledge to proceed further.\\r\\n\\r\\n\\r\\n711 FLAT REFUND STATEMENT\\r\\n<http://s.no/>S.NO<http://s.no/>        DESCRIPTION OF WORK     UNITS/SFT       Nos     RATE    AMOUNT\\r\\n1       Wash Basin including Installations              1       4332    4332\\r\\n2       Kitchen Granite (Material & Installations)                              5080\\r\\n3       Grills                          21650\\r\\nGrand Total                             31062\\r\\nInterior Security Deposit Amount\\r\\n10000\\r\\nAmount to pay   21062\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nShankar.S\\r\\nManager-Customer Relations\\r\\nPh:63669 55995\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n--\\r\\nThanks/Regards\\r\\nRamakrishna\\r\\n7993355226\\r\\n2getherments infra Pvt Ltd\\r\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_filtering_tool(\n",
    "    uid=\"198ca6688975b1b3\",\n",
    "    threadId=None,\n",
    "    sender=None,\n",
    "    recipient=None,\n",
    "    subject=None,\n",
    "    cc=False,\n",
    "    labels=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    body=True,\n",
    "    html=False,\n",
    "    sort_by=\"date_dt\",\n",
    "    sort_order=\"desc\",\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3168e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-08-28 11:50:03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
